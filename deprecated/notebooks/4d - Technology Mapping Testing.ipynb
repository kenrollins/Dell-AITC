{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e0c71d-16d7-49e2-b1d7-1702db1586f0",
   "metadata": {},
   "source": [
    "# Block 1: Imports, Configuration, and Base Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26328c-a86d-4b1a-b30b-0d64c2b587f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Set, Optional, Tuple, Any\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import re\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import psutil\n",
    "import gc\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum, auto\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "import nest_asyncio\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Appely nest_asyncio for Jupyter compatibility\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    # Neo4j connection\n",
    "    NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "    NEO4J_USER = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"kuxFc8HN\"\n",
    "    \n",
    "    # LLM settings\n",
    "    OLLAMA_API = \"http://localhost:11434/api/generate\"\n",
    "    OLLAMA_MODEL = \"mixtral\"\n",
    "    OLLAMA_TIMEOUT = 30\n",
    "\n",
    "    # Processing settings\n",
    "    BATCH_SIZE = 5\n",
    "    SAMPLE_SIZE = 1741  # <-- Add this to process only 10 cases\n",
    "    MEMORY_THRESHOLD_MB = 1000\n",
    "    \n",
    "    # Scoring thresholds for cascading evaluation\n",
    "    class Thresholds:\n",
    "        KEYWORD = 0.2      # If keyword match is strong enough, skip other evaluations\n",
    "        SEMANTIC = 0.3    # If semantic match is strong enough, skip LLM\n",
    "        LLM = 0.5         # Minimum threshold for LLM-based matches\n",
    "        \n",
    "        # Relationship type thresholds\n",
    "        PRIMARY = 0.7     \n",
    "        SECONDARY = 0.5\n",
    "        RELATED = 0.3\n",
    "\n",
    "    # Method weights for final score calculation\n",
    "    METHOD_WEIGHTS = {\n",
    "        'keyword': {\n",
    "            'score_weight': 1.0,\n",
    "            'confidence': 'HIGH'\n",
    "        },\n",
    "        'semantic': {\n",
    "            'score_weight': 0.9,\n",
    "            'confidence': 'MEDIUM'\n",
    "        },\n",
    "        'llm': {\n",
    "            'score_weight': 0.8,\n",
    "            'confidence': 'VARIABLE'  # Will be set by LLM response\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Set up logging with enhanced output\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Change to DEBUG for more verbose logging\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    filename='technology_mapping.log'  # Log to a file\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add custom logging formatter for evaluation method tracking\n",
    "class EvalMethodFormatter(logging.Formatter):\n",
    "    eval_colors = {\n",
    "        'KEYWORD': '\\033[32m',  # Green\n",
    "        'SEMANTIC': '\\033[33m',  # Yellow\n",
    "        'LLM': '\\033[36m',      # Cyan\n",
    "        'RESET': '\\033[0m'\n",
    "    }\n",
    "    \n",
    "    def format(self, record):\n",
    "        if hasattr(record, 'eval_method'):\n",
    "            record.msg = f\"{self.eval_colors[record.eval_method]}{record.eval_method}{self.eval_colors['RESET']} - {record.msg}\"\n",
    "        return super().format(record)\n",
    "\n",
    "# Add handler with custom formatter\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(EvalMethodFormatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(handler)\n",
    "\n",
    "class LRUCache:\n",
    "    \"\"\"Least Recently Used (LRU) cache implementation\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def get(self, key: str) -> any:\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        self.cache.move_to_end(key)\n",
    "        return self.cache[key]\n",
    "\n",
    "    def put(self, key: str, value: any) -> None:\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        else:\n",
    "            if len(self.cache) >= self.capacity:\n",
    "                self.cache.popitem(last=False)\n",
    "        self.cache[key] = value\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all items from cache\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "# Enums for type safety\n",
    "class MatchMethod(Enum):\n",
    "    KEYWORD = auto()\n",
    "    SEMANTIC = auto()\n",
    "    LLM = auto()\n",
    "    NO_MATCH = auto()\n",
    "    ERROR = auto()\n",
    "\n",
    "class RelationType(Enum):\n",
    "    PRIMARY = auto()\n",
    "    SECONDARY = auto()\n",
    "    RELATED = auto()\n",
    "    NO_MATCH = auto()\n",
    "\n",
    "# Data classes for structured results\n",
    "@dataclass\n",
    "class MatchResult:\n",
    "    use_case_name: str\n",
    "    agency: str\n",
    "    abbreviation: str \n",
    "    category_name: str\n",
    "    score: float\n",
    "    method: MatchMethod\n",
    "    relationship_type: RelationType\n",
    "    confidence: float\n",
    "    matched_terms: Optional[List[str]] = None\n",
    "    justification: Optional[str] = None\n",
    "    error: Optional[str] = None\n",
    "\n",
    "def verify_environment():\n",
    "    \"\"\"Verify all required components are available\"\"\"\n",
    "    required_components = {\n",
    "        'neo4j': False,\n",
    "        'sentence_transformers': False,\n",
    "        'ollama': False,\n",
    "        'memory': False\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check Neo4j connection\n",
    "        with GraphDatabase.driver(\n",
    "            Config.NEO4J_URI, \n",
    "            auth=(Config.NEO4J_USER, Config.NEO4J_PASSWORD)\n",
    "        ) as driver:\n",
    "            driver.verify_connectivity()\n",
    "            required_components['neo4j'] = True\n",
    "            logger.info(\"✓ Neo4j connection verified\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Neo4j connection failed: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Check sentence transformers\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        required_components['sentence_transformers'] = True\n",
    "        logger.info(\"✓ Sentence transformers model loaded\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Sentence transformers failed: {str(e)}\")\n",
    "    \n",
    "    try:\n",
    "        # Check Ollama availability\n",
    "        response = requests.get(\n",
    "            Config.OLLAMA_API.replace('/generate', '/version'), \n",
    "            timeout=5\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            required_components['ollama'] = True\n",
    "            logger.info(\"✓ Ollama service available\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"✗ Ollama service unavailable: {str(e)}\")\n",
    "    \n",
    "    # Check available memory\n",
    "    available_memory = psutil.virtual_memory().available / (1024 * 1024)\n",
    "    required_components['memory'] = available_memory > Config.MEMORY_THRESHOLD_MB\n",
    "    if required_components['memory']:\n",
    "        logger.info(f\"✓ Sufficient memory available ({available_memory:.0f}MB)\")\n",
    "    else:\n",
    "        logger.error(f\"✗ Insufficient memory: {available_memory:.0f}MB available, \"\n",
    "                    f\"{Config.MEMORY_THRESHOLD_MB}MB required\")\n",
    "    \n",
    "    # Return overall status\n",
    "    if all(required_components.values()):\n",
    "        logger.info(\"All components verified successfully!\")\n",
    "        return True\n",
    "    else:\n",
    "        failed = [k for k, v in required_components.items() if not v]\n",
    "        logger.error(f\"Verification failed for: {', '.join(failed)}\")\n",
    "        return False\n",
    "\n",
    "# Run initial verification\n",
    "print(\"Verifying environment setup...\")\n",
    "verify_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9f8c2-89ef-4dee-82a0-77818f12c50b",
   "metadata": {},
   "source": [
    "# Block 2: Neo4j Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ac37a-e962-4f59-9e8b-95b99a2f271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class Category:\n",
    "    \"\"\"Data structure for AI technology categories\"\"\"\n",
    "    name: str\n",
    "    definition: str\n",
    "    maturity_level: str\n",
    "    keywords: List[str]\n",
    "    capabilities: List[str]\n",
    "    combined_text: str\n",
    "    keyword_count: int\n",
    "    capability_count: int\n",
    "\n",
    "@dataclass\n",
    "class UseCase:\n",
    "    \"\"\"Data structure for use cases\"\"\"\n",
    "    name: str\n",
    "    agency: str\n",
    "    abbreviation: str\n",
    "    topic_area: str\n",
    "    dev_stage: str\n",
    "    purpose_benefits: str\n",
    "    outputs: List[str]\n",
    "    combined_text: str\n",
    "\n",
    "class Neo4jInterface:\n",
    "    \"\"\"Enhanced Neo4j interface with connection pooling and caching\"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.logger = logging.getLogger('tech_mapper.neo4j')\n",
    "        self._categories: Optional[Dict[str, Category]] = None\n",
    "        self._use_cases: Optional[Dict[str, UseCase]] = None\n",
    "        \n",
    "        # Initialize connection pool\n",
    "        self.driver.verify_connectivity()\n",
    "        \n",
    "        # Cache settings\n",
    "        self._cache_timestamp = None\n",
    "        self._cache_lifetime = 3600  # 1 hour cache lifetime\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Safely close Neo4j connection\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            self.driver = None\n",
    "            \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "    \n",
    "    @property\n",
    "    def categories(self) -> Dict[str, Category]:\n",
    "        \"\"\"Cached access to AI technology categories\"\"\"\n",
    "        if self._categories is None or self._cache_expired:\n",
    "            self._categories = self._fetch_categories()\n",
    "            self._cache_timestamp = time.time()\n",
    "        return self._categories\n",
    "    \n",
    "    @property\n",
    "    def use_cases(self) -> Dict[str, UseCase]:\n",
    "        \"\"\"Cached access to use cases\"\"\"\n",
    "        if self._use_cases is None or self._cache_expired:\n",
    "            self._use_cases = self._fetch_use_cases()\n",
    "            self._cache_timestamp = time.time()\n",
    "        return self._use_cases\n",
    "    \n",
    "    @property\n",
    "    def _cache_expired(self) -> bool:\n",
    "        \"\"\"Check if cache needs refresh\"\"\"\n",
    "        if self._cache_timestamp is None:\n",
    "            return True\n",
    "        return (time.time() - self._cache_timestamp) > self._cache_lifetime\n",
    "    \n",
    "    def _execute_query(self, query: str, parameters: Optional[Dict] = None) -> List[Dict]:\n",
    "        \"\"\"Execute Neo4j query with retry logic and error handling\"\"\"\n",
    "        max_retries = 3\n",
    "        retry_delay = 1\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                with self.driver.session() as session:\n",
    "                    result = session.run(query, parameters or {})\n",
    "                    return [record.data() for record in result]\n",
    "                    \n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.logger.error(f\"Query failed after {max_retries} attempts: {str(e)}\")\n",
    "                    raise\n",
    "                    \n",
    "                self.logger.warning(\n",
    "                    f\"Query attempt {attempt + 1} failed: {str(e)}. \"\n",
    "                    f\"Retrying in {retry_delay}s...\"\n",
    "                )\n",
    "                time.sleep(retry_delay)\n",
    "                retry_delay *= 2  # Exponential backoff\n",
    "    \n",
    "    def _fetch_categories(self) -> Dict[str, Category]:\n",
    "        \"\"\"Fetch all AI technology categories with enhanced context\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory)\n",
    "        OPTIONAL MATCH (c)-[:TAGGED_WITH]->(k:Keyword)\n",
    "        OPTIONAL MATCH (c)-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "        WITH c,\n",
    "             collect(DISTINCT k.name) as keywords,\n",
    "             collect(DISTINCT cap.name) as capabilities,\n",
    "             c.definition + ' ' + \n",
    "             reduce(s = '', x IN collect(DISTINCT k.name) | s + ' ' + x) + ' ' +\n",
    "             reduce(s = '', x IN collect(DISTINCT cap.name) | s + ' ' + x) as combined_text\n",
    "        RETURN \n",
    "            c.name as name,\n",
    "            c.definition as definition,\n",
    "            c.maturity_level as maturity_level,\n",
    "            keywords,\n",
    "            capabilities,\n",
    "            combined_text,\n",
    "            size(keywords) as keyword_count,\n",
    "            size(capabilities) as capability_count\n",
    "        ORDER BY c.name\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            results = self._execute_query(query)\n",
    "            categories = {}\n",
    "            \n",
    "            for row in results:\n",
    "                category = Category(\n",
    "                    name=row['name'],\n",
    "                    definition=row['definition'],\n",
    "                    maturity_level=row['maturity_level'],\n",
    "                    keywords=row['keywords'],\n",
    "                    capabilities=row['capabilities'],\n",
    "                    combined_text=row['combined_text'],\n",
    "                    keyword_count=row['keyword_count'],\n",
    "                    capability_count=row['capability_count']\n",
    "                )\n",
    "                categories[category.name] = category\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Loaded {len(categories)} categories with \"\n",
    "                f\"{sum(c.keyword_count for c in categories.values())} total keywords\"\n",
    "            )\n",
    "            return categories\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to fetch categories: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _fetch_use_cases(self) -> Dict[str, UseCase]:\n",
    "        \"\"\"Fetch all use cases with batch processing\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (u:UseCase)\n",
    "        OPTIONAL MATCH (u)-[:HAS_PURPOSE]->(p:PurposeBenefit)\n",
    "        OPTIONAL MATCH (u)-[:PRODUCES]->(o:Output)\n",
    "        OPTIONAL MATCH (u)<-[:HAS_USE_CASE]-(a:Agency)\n",
    "        WITH u, a,\n",
    "             collect(DISTINCT p.description) as purposes,\n",
    "             collect(DISTINCT o.description) as outputs,\n",
    "             u.purpose_benefits + ' ' +\n",
    "             reduce(s = '', x IN collect(DISTINCT p.description) | s + ' ' + x) + ' ' +\n",
    "             reduce(s = '', x IN collect(DISTINCT o.description) | s + ' ' + x) as combined_text\n",
    "        RETURN \n",
    "            u.name as name,\n",
    "            u.agency as agency,\n",
    "            coalesce(a.abbreviation, '') as abbreviation,\n",
    "            u.topic_area as topic_area,\n",
    "            u.dev_stage as dev_stage,\n",
    "            u.purpose_benefits as purpose_benefits,\n",
    "            outputs,\n",
    "            combined_text\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            results = self._execute_query(query)\n",
    "            use_cases = {}\n",
    "            \n",
    "            for row in results:\n",
    "                use_case = UseCase(\n",
    "                    name=row['name'],\n",
    "                    agency=row['agency'],\n",
    "                    abbreviation=row['abbreviation'] or '',\n",
    "                    topic_area=row['topic_area'],\n",
    "                    dev_stage=row['dev_stage'],\n",
    "                    purpose_benefits=row['purpose_benefits'] or '',\n",
    "                    outputs=row['outputs'],\n",
    "                    combined_text=row['combined_text']\n",
    "                )\n",
    "                use_cases[f\"{use_case.name}|{use_case.agency}\"] = use_case\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(use_cases)} unmapped use cases\")\n",
    "            return use_cases\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to fetch use cases: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    import csv\n",
    "\n",
    "    def save_match(self, match: MatchResult) -> bool:\n",
    "        \"\"\"Save match results to a single CSV file for the entire run\"\"\"\n",
    "        try:\n",
    "            # Create output directory if it doesn't exist\n",
    "            output_dir = Path(\"output\")\n",
    "            output_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            # Use a consistent filename for the entire run\n",
    "            run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            output_file = output_dir / f\"technology_mapping_results_{run_timestamp}.csv\"\n",
    "            \n",
    "            # Check if file exists to determine if we need to write headers\n",
    "            file_exists = output_file.exists()\n",
    "            \n",
    "            # Open file in append mode\n",
    "            with open(output_file, 'a', newline='') as csvfile:\n",
    "                fieldnames = [\n",
    "                    'use_case_name', \n",
    "                    'agency', \n",
    "                    'abbreviation',\n",
    "                    'category_name', \n",
    "                    'keyword_score', \n",
    "                    'semantic_score', \n",
    "                    'llm_score',\n",
    "                    'final_score', \n",
    "                    'match_method', \n",
    "                    'relationship_type', \n",
    "                    'confidence', \n",
    "                    'matched_keywords', \n",
    "                    'justification',\n",
    "                    'error'\n",
    "                ]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                \n",
    "                # Write headers if file is new\n",
    "                if not file_exists:\n",
    "                    writer.writeheader()\n",
    "                \n",
    "                # Convert match to dictionary with more detailed scoring\n",
    "                match_dict = {\n",
    "                    'use_case_name': match.use_case_name,\n",
    "                    'agency': match.agency,\n",
    "                    'abbreviation': match.abbreviation,\n",
    "                    'category_name': match.category_name,\n",
    "                    'keyword_score': match.score if match.method == MatchMethod.KEYWORD else 0.0,\n",
    "                    'semantic_score': match.score if match.method == MatchMethod.SEMANTIC else 0.0,\n",
    "                    'llm_score': match.score if match.method == MatchMethod.LLM else 0.0,\n",
    "                    'final_score': match.score,\n",
    "                    'match_method': match.method.name,\n",
    "                    'relationship_type': match.relationship_type.name,\n",
    "                    'confidence': match.confidence,\n",
    "                    'matched_keywords': ', '.join(match.matched_terms) if match.matched_terms else '',\n",
    "                    'justification': match.justification,\n",
    "                    'error': match.error if hasattr(match, 'error') else ''\n",
    "                }\n",
    "                \n",
    "                # Write the match\n",
    "                writer.writerow(match_dict)\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to save match to CSV: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def get_match_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about current category matches\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (u:UseCase)\n",
    "        RETURN count(DISTINCT u) as total_use_cases\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = self._execute_query(query)[0]\n",
    "            \n",
    "            return {\n",
    "                'total_use_cases': result['total_use_cases'],\n",
    "                'total_matches': 0,\n",
    "                'matched_categories': 0,\n",
    "                'match_methods': {},\n",
    "                'relationship_types': {},\n",
    "                'completion_rate': 0\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to get match statistics: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Test Neo4j interface\n",
    "    with Neo4jInterface(\n",
    "        Config.NEO4J_URI,\n",
    "        Config.NEO4J_USER,\n",
    "        Config.NEO4J_PASSWORD\n",
    "    ) as db:\n",
    "        # Print some basic stats\n",
    "        print(f\"Categories loaded: {len(db.categories)}\")\n",
    "        print(f\"Use cases loaded: {len(db.use_cases)}\")\n",
    "        \n",
    "        # Print match statistics\n",
    "        stats = db.get_match_statistics()\n",
    "        print(\"\\nMatch Statistics:\")\n",
    "        print(f\"Completion Rate: {stats['completion_rate']:.1%}\")\n",
    "        print(\"\\nMatch Methods:\")\n",
    "        for method, count in stats['match_methods'].items():\n",
    "            print(f\"- {method}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c580bc8a-8632-467e-b17d-ae9126143456",
   "metadata": {},
   "source": [
    "# Block 3: Enhanced text processing and matching with memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b02376-69e3-4854-bd4f-fc37bb345865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryManager:\n",
    "    \"\"\"Manages memory usage and garbage collection\"\"\"\n",
    "    \n",
    "    def __init__(self, threshold_mb: int = 1000):\n",
    "        self.threshold_mb = threshold_mb\n",
    "        self.last_check = time.time()\n",
    "        self.check_interval = 60  # Check every minute\n",
    "        self.logger = logging.getLogger('tech_mapper.memory')\n",
    "        \n",
    "    def check_memory(self) -> bool:\n",
    "        \"\"\"Check memory usage and clean up if needed\"\"\"\n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_check < self.check_interval:\n",
    "            return True\n",
    "            \n",
    "        try:\n",
    "            memory_used = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "            self.last_check = current_time\n",
    "            \n",
    "            if memory_used > self.threshold_mb:\n",
    "                self.logger.warning(\n",
    "                    f\"Memory usage ({memory_used:.0f}MB) exceeded threshold \"\n",
    "                    f\"({self.threshold_mb}MB). Running garbage collection.\"\n",
    "                )\n",
    "                gc.collect()\n",
    "                \n",
    "                # Check if gc helped\n",
    "                memory_after = psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "                if memory_after > self.threshold_mb:\n",
    "                    self.logger.error(\n",
    "                        f\"Memory usage still high ({memory_after:.0f}MB) \"\n",
    "                        \"after garbage collection!\"\n",
    "                    )\n",
    "                    return False\n",
    "                    \n",
    "                self.logger.info(\n",
    "                    f\"Memory reduced from {memory_used:.0f}MB to {memory_after:.0f}MB\"\n",
    "                )\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Memory check failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Handles text processing and matching with memory management\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.memory_manager = MemoryManager(Config.MEMORY_THRESHOLD_MB)\n",
    "        self.embedding_cache = {}\n",
    "        self.cache_hits = 0\n",
    "        self.cache_misses = 0\n",
    "        self.logger = logging.getLogger('tech_mapper.text')\n",
    "        \n",
    "        # Pre-compile regex patterns\n",
    "        self.cleanup_pattern = re.compile(r'\\s+')\n",
    "        self.split_pattern = re.compile(r'[;,\\n]')\n",
    "    \n",
    "    def cleanup_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase and normalize whitespace\n",
    "        text = text.lower()\n",
    "        text = self.cleanup_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    @lru_cache(maxsize=10000)\n",
    "    \n",
    "    def get_keyword_matches(self, text: str, keywords: Tuple[str], threshold: float = 0.3) -> Tuple[float, Set[str]]:\n",
    "        if not text or not keywords:\n",
    "            return 0.0, set()\n",
    "            \n",
    "        text = self.cleanup_text(text)\n",
    "        matches = set()\n",
    "        matched_details = {}\n",
    "        \n",
    "        # Track partial matches for more nuanced scoring\n",
    "        partial_matches = 0\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            keyword = keyword.lower()\n",
    "            \n",
    "            # Exact match\n",
    "            if keyword in text:\n",
    "                matches.add(keyword)\n",
    "                matched_details[keyword] = 'exact'\n",
    "                continue\n",
    "            \n",
    "            # Check for partial matches\n",
    "            keyword_terms = set(keyword.split())\n",
    "            \n",
    "            # If keyword has multiple terms, check for partial match\n",
    "            if len(keyword_terms) > 1:\n",
    "                matched_terms = [term for term in keyword_terms if term in text]\n",
    "                \n",
    "                # Require at least half the terms to match\n",
    "                if len(matched_terms) >= len(keyword_terms) / 2:\n",
    "                    matches.add(keyword)\n",
    "                    partial_matches += len(matched_terms) / len(keyword_terms)\n",
    "                    matched_details[keyword] = f'partial: {matched_terms}'\n",
    "        \n",
    "        # Log matching details\n",
    "        if matched_details:\n",
    "            self.logger.info(f\"Keyword Matches: {matched_details}\")\n",
    "        \n",
    "        # Modified scoring to be more lenient\n",
    "        total_matches = len(matches) + (partial_matches * 0.7)  # Increase partial match weight\n",
    "        score = min(1.0, total_matches / (len(keywords) * 0.5))  # Only require matching half of keywords for max score\n",
    "        \n",
    "        return score, matches\n",
    "        \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get or compute text embedding with memory management\"\"\"\n",
    "        if not text:\n",
    "            return self.model.encode(\"\")\n",
    "        \n",
    "        # Check cache\n",
    "        text = self.cleanup_text(text)\n",
    "        if text in self.embedding_cache:\n",
    "            self.cache_hits += 1\n",
    "            return self.embedding_cache[text]\n",
    "        \n",
    "        # Check memory before computing new embedding\n",
    "        if not self.memory_manager.check_memory():\n",
    "            self.logger.warning(\"Memory threshold exceeded, clearing embedding cache\")\n",
    "            self.embedding_cache.clear()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Compute new embedding\n",
    "        self.cache_misses += 1\n",
    "        try:\n",
    "            embedding = self.model.encode(text, convert_to_tensor=True)\n",
    "            self.embedding_cache[text] = embedding\n",
    "            return embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to compute embedding: {str(e)}\")\n",
    "            return self.model.encode(\"\")  # Return empty embedding on error\n",
    "    \n",
    "    def get_semantic_similarity(\n",
    "        self, \n",
    "        text1: str, \n",
    "        text2: str,\n",
    "        threshold: float = 0.75\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate semantic similarity between texts\"\"\"\n",
    "        try:\n",
    "            # Log semantic similarity computation details\n",
    "            self.logger.info(\"Computing Semantic Similarity\")\n",
    "            self.logger.debug(f\"Text 1: {text1}\")\n",
    "            self.logger.debug(f\"Text 2: {text2}\")\n",
    "            self.logger.debug(f\"Threshold: {threshold}\")\n",
    "            \n",
    "            # Get embeddings\n",
    "            embedding1 = self.get_embedding(text1)\n",
    "            embedding2 = self.get_embedding(text2)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = float(\n",
    "                util.pytorch_cos_sim(embedding1, embedding2)[0][0]\n",
    "            )\n",
    "            \n",
    "            # Log strong matches\n",
    "            if similarity >= threshold:\n",
    "                self.logger.info(\n",
    "                    f\"Strong semantic match ({similarity:.2f})\",\n",
    "                    extra={'method': 'SEMANTIC'}\n",
    "                )\n",
    "            \n",
    "            return similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Semantic similarity failed: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_match(\n",
    "        self, \n",
    "        use_case: UseCase, \n",
    "        category: Category,\n",
    "        thresholds: Config.Thresholds = Config.Thresholds\n",
    "    ) -> MatchResult:\n",
    "        try:\n",
    "            # Step 1: Quick keyword matching\n",
    "            keyword_score, matched_terms = self.get_keyword_matches(\n",
    "                use_case.combined_text,\n",
    "                tuple(category.keywords + category.capabilities),\n",
    "                threshold=thresholds.KEYWORD\n",
    "            )\n",
    "            \n",
    "            if keyword_score >= thresholds.KEYWORD:\n",
    "                return MatchResult(\n",
    "                    use_case_name=use_case.name,\n",
    "                    agency=use_case.agency,\n",
    "                    abbreviation=use_case.abbreviation or '',\n",
    "                    category_name=category.name,\n",
    "                    score=keyword_score,\n",
    "                    method=MatchMethod.KEYWORD,\n",
    "                    relationship_type=self._get_relationship_type(keyword_score),\n",
    "                    confidence=keyword_score,\n",
    "                    matched_terms=list(matched_terms),\n",
    "                    justification=f\"Keyword match score: {keyword_score:.2f}\"\n",
    "                )\n",
    "            \n",
    "            # Step 2: Try semantic matching if keywords weren't sufficient\n",
    "            semantic_score = self.get_semantic_similarity(\n",
    "                use_case.combined_text,\n",
    "                category.combined_text,\n",
    "                threshold=thresholds.SEMANTIC\n",
    "            )\n",
    "            \n",
    "            if semantic_score >= thresholds.SEMANTIC:\n",
    "                return MatchResult(\n",
    "                    use_case_name=use_case.name,\n",
    "                    agency=use_case.agency,\n",
    "                    abbreviation=use_case.abbreviation or '',\n",
    "                    category_name=category.name,\n",
    "                    score=semantic_score,\n",
    "                    method=MatchMethod.SEMANTIC,\n",
    "                    relationship_type=self._get_relationship_type(semantic_score),\n",
    "                    confidence=semantic_score * 0.9,\n",
    "                    matched_terms=list(matched_terms) if matched_terms else None,\n",
    "                    justification=f\"Semantic match score: {semantic_score:.2f}\"\n",
    "                )\n",
    "            \n",
    "            # Step 3: Potentially trigger LLM evaluation\n",
    "            best_score = max(keyword_score, semantic_score)\n",
    "            \n",
    "            # Trigger LLM if the score is above the RELATED threshold\n",
    "            if best_score > thresholds.RELATED:\n",
    "                # Log that we're considering LLM evaluation\n",
    "                self.logger.info(\n",
    "                    f\"Potential match detected for use case {use_case.name} \"\n",
    "                    f\"in category {category.name}. Best score: {best_score:.2f}\"\n",
    "                )\n",
    "                \n",
    "                return MatchResult(\n",
    "                    use_case_name=use_case.name,\n",
    "                    agency=use_case.agency,\n",
    "                    abbreviation=use_case.abbreviation or '',\n",
    "                    category_name=category.name,\n",
    "                    score=best_score,\n",
    "                    method=MatchMethod.NO_MATCH,\n",
    "                    relationship_type=RelationType.RELATED,\n",
    "                    confidence=best_score,\n",
    "                    matched_terms=list(matched_terms) if matched_terms else None,\n",
    "                    justification=\"Potential match - recommended for LLM validation\"\n",
    "                )\n",
    "            \n",
    "            # No significant match found\n",
    "            return MatchResult(\n",
    "                use_case_name=use_case.name,\n",
    "                agency=use_case.agency,\n",
    "                abbreviation=use_case.abbreviation or '',\n",
    "                category_name=category.name,\n",
    "                score=best_score,\n",
    "                method=MatchMethod.NO_MATCH,\n",
    "                relationship_type=RelationType.NO_MATCH,\n",
    "                confidence=0.0,\n",
    "                justification=\"No significant match found\"\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Evaluation failed: {str(e)}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return MatchResult(\n",
    "                use_case_name=use_case.name,\n",
    "                agency=use_case.agency,\n",
    "                abbreviation=use_case.abbreviation or '',\n",
    "                category_name=category.name,\n",
    "                score=0.0,\n",
    "                method=MatchMethod.ERROR,\n",
    "                relationship_type=RelationType.NO_MATCH,\n",
    "                confidence=0.0,\n",
    "                error=error_msg\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def _get_relationship_type(self, score: float) -> RelationType:\n",
    "        \"\"\"Determine relationship type based on score\"\"\"\n",
    "        if score >= Config.Thresholds.PRIMARY:\n",
    "            return RelationType.PRIMARY\n",
    "        elif score >= Config.Thresholds.SECONDARY:\n",
    "            return RelationType.SECONDARY\n",
    "        elif score >= Config.Thresholds.RELATED:\n",
    "            return RelationType.RELATED\n",
    "        return RelationType.NO_MATCH\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        total_requests = self.cache_hits + self.cache_misses\n",
    "        if total_requests == 0:\n",
    "            return {\n",
    "                'cache_hits': 0,\n",
    "                'cache_misses': 0,\n",
    "                'hit_rate': 0,\n",
    "                'cache_size': len(self.embedding_cache)\n",
    "            }\n",
    "            \n",
    "        return {\n",
    "            'cache_hits': self.cache_hits,\n",
    "            'cache_misses': self.cache_misses,\n",
    "            'hit_rate': self.cache_hits / total_requests * 100,\n",
    "            'cache_size': len(self.embedding_cache)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    processor = TextProcessor()\n",
    "    \n",
    "    # Test text processing\n",
    "    text1 = \"AI-powered data analytics for predictive maintenance\"\n",
    "    text2 = \"Machine learning models for equipment failure prediction\"\n",
    "    \n",
    "    # Get similarity\n",
    "    similarity = processor.get_semantic_similarity(text1, text2)\n",
    "    print(f\"Semantic similarity: {similarity:.2f}\")\n",
    "    \n",
    "    # Test memory management\n",
    "    print(\"\\nMemory usage:\")\n",
    "    for _ in range(100):\n",
    "        processor.get_embedding(f\"Test text {_}\" * 100)\n",
    "        if _ % 10 == 0:\n",
    "            stats = processor.get_cache_stats()\n",
    "            print(f\"Cache size: {stats['cache_size']}, Hit rate: {stats['hit_rate']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc71c5-58d5-4273-9277-becddb4b7e54",
   "metadata": {},
   "source": [
    "# Block 4: Enhanced LLM integration with robust error handling and response parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2acfb-5930-46f7-9dcd-192d79f337d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import backoff\n",
    "from typing import Optional, Dict, Any\n",
    "import json\n",
    "import asyncio\n",
    "from dataclasses import asdict\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    \"\"\"Structured LLM response\"\"\"\n",
    "    is_match: bool\n",
    "    confidence: float\n",
    "    primary_category: bool\n",
    "    justification: str\n",
    "    suggested_categories: List[str]\n",
    "    error: Optional[str] = None\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Enhanced LLM interface with retry logic and response validation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        api_url: str, \n",
    "        model: str,\n",
    "        timeout: int = 30,\n",
    "        max_retries: int = 2\n",
    "    ):\n",
    "        self.api_url = api_url\n",
    "        self.model = model\n",
    "        self.timeout = aiohttp.ClientTimeout(total=timeout)\n",
    "        self.max_retries = max_retries\n",
    "        self.logger = logging.getLogger('tech_mapper.llm')\n",
    "        \n",
    "        # Template management\n",
    "        self._load_templates()\n",
    "        \n",
    "        # Session management\n",
    "        self._session: Optional[aiohttp.ClientSession] = None\n",
    "        self._results_cache = {}\n",
    "    \n",
    "    def _load_templates(self):\n",
    "        \"\"\"Load prompt templates\"\"\"\n",
    "        self.templates = {\n",
    "            'category_match': \"\"\"You are a strict JSON-only response system performing AI technology category matching. \n",
    "            Evaluate if this use case matches the category based on the provided information and previous evaluations.\n",
    "            \n",
    "            Format your response as a JSON object with ONLY these fields:\n",
    "            {\n",
    "                \"is_match\": boolean,\n",
    "                \"confidence\": float between 0 and 1,\n",
    "                \"primary_category\": boolean,\n",
    "                \"justification\": \"string explanation\",\n",
    "                \"suggested_categories\": [\"Category1\", \"Category2\", ...]\n",
    "            }\n",
    "            \n",
    "            Previous Evaluation Methods:\n",
    "            - Keyword Match Score: {keyword_score:.2f}\n",
    "            - Semantic Match Score: {semantic_score:.2f}\n",
    "            - Keywords Found: {matched_keywords}\n",
    "            \n",
    "            Category to Evaluate:\n",
    "            Name: {category_name}\n",
    "            Definition: {category_definition}\n",
    "            Primary Capabilities: {capabilities}\n",
    "            \n",
    "            Use Case:\n",
    "            Name: {use_case_name}\n",
    "            Agency: {agency}\n",
    "            Purpose: {purpose}\n",
    "            Outputs: {outputs}\n",
    "            \n",
    "            Evaluate if this use case matches this category and return ONLY the required JSON object.\n",
    "            \"\"\"\n",
    "        }\n",
    "    \n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Async context manager entry\"\"\"\n",
    "        self._session = aiohttp.ClientSession(timeout=self.timeout)\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc, tb):\n",
    "        \"\"\"Async context manager exit\"\"\"\n",
    "        if self._session:\n",
    "            await self._session.close()\n",
    "            self._session = None\n",
    "    \n",
    "    def _build_prompt(\n",
    "        self, \n",
    "        use_case: UseCase, \n",
    "        category: Category, \n",
    "        prev_scores: Dict[str, Any]\n",
    "    ) -> str:\n",
    "        \"\"\"Build prompt from template\"\"\"\n",
    "        # Safely handle matched terms\n",
    "        matched_terms = prev_scores.get('matched_terms', [])\n",
    "        matched_terms_str = ', '.join(matched_terms) if matched_terms else 'None'\n",
    "    \n",
    "        return f\"\"\"You are a strict JSON-only response system performing AI technology category matching. \n",
    "        Carefully evaluate if this use case matches the category based on the provided information.\n",
    "    \n",
    "        IMPORTANT: Your ENTIRE response must be a valid JSON object with these EXACT fields:\n",
    "        {{\n",
    "            \"is_match\": true or false,\n",
    "            \"confidence\": a float between 0 and 1,\n",
    "            \"primary_category\": true or false,\n",
    "            \"justification\": \"a clear explanation of your reasoning\",\n",
    "            \"suggested_categories\": [\"Category1\", \"Category2\"]\n",
    "        }}\n",
    "    \n",
    "        Previous Evaluation Methods:\n",
    "        - Keyword Match Score: {prev_scores.get('keyword_score', 0):.2f}\n",
    "        - Semantic Match Score: {prev_scores.get('semantic_score', 0):.2f}\n",
    "        - Keywords Found: {matched_terms_str}\n",
    "        \n",
    "        Category to Evaluate:\n",
    "        Name: {category.name}\n",
    "        Definition: {category.definition}\n",
    "        Primary Capabilities: {', '.join(category.capabilities[:5])}\n",
    "        \n",
    "        Use Case:\n",
    "        Name: {use_case.name}\n",
    "        Agency: {use_case.agency}\n",
    "        Purpose: {use_case.purpose_benefits[:500]}\n",
    "        Outputs: {', '.join(use_case.outputs[:5])}\n",
    "        \n",
    "        Evaluate carefully and return ONLY the required JSON object.\"\"\"\n",
    "\n",
    "    \n",
    "    def _get_cache_key(\n",
    "        self, \n",
    "        use_case: UseCase, \n",
    "        category: Category,\n",
    "        scores: Dict\n",
    "    ) -> str:\n",
    "        \"\"\"Create cache key for results\"\"\"\n",
    "        key_parts = [\n",
    "            f\"uc:{use_case.name}\",\n",
    "            f\"ag:{use_case.agency}\",\n",
    "            f\"cat:{category.name}\",\n",
    "            f\"scores:{scores.get('keyword_score', 0):.2f}|{scores.get('semantic_score', 0):.2f}\"\n",
    "        ]\n",
    "        return \"|\".join(key_parts)\n",
    "    \n",
    "    @backoff.on_exception(\n",
    "        backoff.expo,\n",
    "        (aiohttp.ClientError, asyncio.TimeoutError),\n",
    "        max_tries=3\n",
    "    )\n",
    "    async def _make_request(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: float = 0.1\n",
    "    ) -> Dict:\n",
    "        \"\"\"Make request to LLM API with retries\"\"\"\n",
    "        if not self._session:\n",
    "            raise RuntimeError(\"Session not initialized - use async context manager\")\n",
    "            \n",
    "        request_data = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.1,\n",
    "            \"top_k\": 10,\n",
    "            \"num_predict\": 100,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        async with self._session.post(\n",
    "            self.api_url,\n",
    "            json=request_data\n",
    "        ) as response:\n",
    "            response.raise_for_status()\n",
    "            return await response.json()\n",
    "    \n",
    "    def _extract_json(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"Extract and validate JSON from LLM response\"\"\"\n",
    "        try:\n",
    "            # Clean up the text to remove potential leading/trailing whitespace or newlines\n",
    "            text = text.strip()\n",
    "            \n",
    "            # Find JSON object bounds with more flexible parsing\n",
    "            start = text.find('{')\n",
    "            end = text.rfind('}') + 1\n",
    "            \n",
    "            if start == -1 or end == 0:\n",
    "                # Try to handle potentially malformed JSON\n",
    "                try:\n",
    "                    # Attempt to parse the entire text as JSON\n",
    "                    data = json.loads(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    raise ValueError(\"No valid JSON object found in response\")\n",
    "            else:\n",
    "                # Extract and parse JSON substring\n",
    "                json_str = text[start:end]\n",
    "                try:\n",
    "                    data = json.loads(json_str)\n",
    "                except json.JSONDecodeError:\n",
    "                    # If substring parsing fails, try parsing the entire text\n",
    "                    try:\n",
    "                        data = json.loads(text)\n",
    "                    except json.JSONDecodeError:\n",
    "                        raise ValueError(\"Failed to parse JSON from response\")\n",
    "        \n",
    "            # Validate required fields\n",
    "            required_fields = {\n",
    "                'is_match': bool,\n",
    "                'confidence': float,\n",
    "                'primary_category': bool,\n",
    "                'justification': str,\n",
    "                'suggested_categories': list\n",
    "            }\n",
    "            \n",
    "            for field, field_type in required_fields.items():\n",
    "                if field not in data:\n",
    "                    raise ValueError(f\"Missing required field: {field}\")\n",
    "                if not isinstance(data[field], field_type):\n",
    "                    raise ValueError(\n",
    "                        f\"Invalid type for {field}: \"\n",
    "                        f\"expected {field_type}, got {type(data[field])}\"\n",
    "                    )\n",
    "        \n",
    "            # Validate confidence range\n",
    "            if not 0 <= data['confidence'] <= 1:\n",
    "                raise ValueError(\n",
    "                    f\"Confidence out of range: {data['confidence']}\"\n",
    "                )\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.error(f\"JSON parsing failed: {str(e)}\\nText: {text}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"JSON extraction failed: {str(e)}\\nText: {text}\")\n",
    "            return None\n",
    "    \n",
    "    def _create_fallback_response(\n",
    "        self,\n",
    "        error_msg: str,\n",
    "        category: Category\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Create fallback response for errors\"\"\"\n",
    "        return LLMResponse(\n",
    "            is_match=False,\n",
    "            confidence=0.0,\n",
    "            primary_category=False,\n",
    "            justification=f\"Error occurred: {error_msg}\",\n",
    "            suggested_categories=[category.name],\n",
    "            error=error_msg\n",
    "        )\n",
    "    \n",
    "    async def evaluate_match(\n",
    "        self,\n",
    "        use_case: UseCase,\n",
    "        category: Category,\n",
    "        prev_scores: Dict[str, Any]\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"Evaluate category match using LLM\"\"\"\n",
    "\n",
    "        self.logger.info(f\"LLM Evaluation Started\")\n",
    "        self.logger.info(f\"Use Case: {use_case.name}\")\n",
    "        self.logger.info(f\"Category: {category.name}\")\n",
    "        self.logger.info(f\"Previous Scores: {prev_scores}\")\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = self._get_cache_key(use_case, category, prev_scores)\n",
    "        if cache_key in self._results_cache:\n",
    "            return self._results_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Build and send prompt\n",
    "            prompt = self._build_prompt(use_case, category, prev_scores)\n",
    "\n",
    "            # Log the full prompt for inspection\n",
    "            self.logger.debug(f\"Generated Prompt:\\n{prompt}\")\n",
    "            \n",
    "            response = await self._make_request(prompt)\n",
    "\n",
    "            # Log the raw response\n",
    "            self.logger.info(f\"Raw LLM Response: {response}\")\n",
    "            \n",
    "            if 'response' not in response:\n",
    "                raise ValueError(\"No response field in LLM output\")\n",
    "                \n",
    "            # Extract and validate JSON\n",
    "            result = self._extract_json(response['response'])\n",
    "\n",
    "            # Log the extracted result\n",
    "            self.logger.info(f\"Extracted Result: {result}\")\n",
    "        \n",
    "            if not result:\n",
    "                raise ValueError(\"Failed to extract valid JSON from response\")\n",
    "            \n",
    "            # Create structured response\n",
    "            llm_response = LLMResponse(\n",
    "                is_match=result['is_match'],\n",
    "                confidence=result['confidence'],\n",
    "                primary_category=result['primary_category'],\n",
    "                justification=result['justification'],\n",
    "                suggested_categories=result['suggested_categories']\n",
    "            )\n",
    "            \n",
    "            # Cache result\n",
    "            self._results_cache[cache_key] = llm_response\n",
    "            \n",
    "            # Log strong matches\n",
    "            if llm_response.is_match and llm_response.confidence >= 0.8:\n",
    "                self.logger.info(\n",
    "                    f\"LLM confirmed match with {llm_response.confidence:.2f} confidence\",\n",
    "                    extra={'method': 'LLM'}\n",
    "                )\n",
    "            \n",
    "            return llm_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"LLM evaluation failed: {str(e)}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return self._create_fallback_response(error_msg, category)\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        return {\n",
    "            'cache_size': len(self._results_cache),\n",
    "            'cached_evaluations': len(self._results_cache)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "async def test_llm():\n",
    "    async with LLMInterface(\n",
    "        Config.OLLAMA_API,\n",
    "        Config.OLLAMA_MODEL\n",
    "    ) as llm:\n",
    "        # Create test data\n",
    "        use_case = UseCase(\n",
    "            name=\"AI-powered predictive maintenance\",\n",
    "            agency=\"TEST\",\n",
    "            abbreviation=\"TEST\",\n",
    "            topic_area=\"Technology\",\n",
    "            dev_stage=\"Production\",\n",
    "            purpose_benefits=\"Predict equipment failures using machine learning\",\n",
    "            outputs=[\"Failure predictions\", \"Maintenance schedules\"],\n",
    "            combined_text=\"AI-powered predictive maintenance system...\"\n",
    "        )\n",
    "        \n",
    "        category = Category(\n",
    "            name=\"Predictive & Pattern Analytics\",\n",
    "            definition=\"Advanced analytical and predictive systems\",\n",
    "            maturity_level=\"Mature\",\n",
    "            keywords=[\"predictive analytics\", \"machine learning\"],\n",
    "            capabilities=[\"failure prediction\", \"pattern detection\"],\n",
    "            combined_text=\"Predictive analytics and pattern detection...\",\n",
    "            keyword_count=2,\n",
    "            capability_count=2\n",
    "        )\n",
    "        \n",
    "        # Test evaluation\n",
    "        prev_scores = {\n",
    "            'keyword_score': 0.7,\n",
    "            'semantic_score': 0.8,\n",
    "            'matched_terms': ['predictive', 'analytics']\n",
    "        }\n",
    "        \n",
    "        result = await llm.evaluate_match(use_case, category, prev_scores)\n",
    "        print(f\"Match Result: {asdict(result)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(test_llm())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd0e54-6213-4351-8185-fe4196b4b452",
   "metadata": {},
   "source": [
    "# Block 5: Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a6df19-b63a-401e-8ffb-b231cb50be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Optional, Set, Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import csv\n",
    "\n",
    "# Ensure these imports match your existing imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Import your existing classes and configurations\n",
    "from typing import List, Dict, Set, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum, auto\n",
    "\n",
    "class TechnologyMappingProcessor:\n",
    "    def __init__(\n",
    "        self, \n",
    "        sample_size: Optional[int] = None,\n",
    "        save_intermediates: bool = True\n",
    "    ):\n",
    "        # Configuration\n",
    "        self.neo4j_uri = Config.NEO4J_URI\n",
    "        self.neo4j_user = Config.NEO4J_USER\n",
    "        self.neo4j_password = Config.NEO4J_PASSWORD\n",
    "        \n",
    "        self.sample_size = sample_size if sample_size is not None else Config.SAMPLE_SIZE\n",
    "        self.batch_size = Config.BATCH_SIZE\n",
    "        \n",
    "        # Logging\n",
    "        self.logger = logging.getLogger('tech_mapper.processor')\n",
    "        \n",
    "        # Components\n",
    "        self.db = None\n",
    "        self.text_processor = None\n",
    "        self.llm = None\n",
    "        \n",
    "        # Results tracking\n",
    "        self.start_time = None\n",
    "        self.stats = {}\n",
    "        self.results = []\n",
    "        self.save_intermediates = save_intermediates\n",
    "        \n",
    "        # Output directory\n",
    "        self.output_dir = Path(\"output\")\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    def initialize(self) -> bool:\n",
    "        \"\"\"Initialize processing components\"\"\"\n",
    "        try:\n",
    "            # Create database connection\n",
    "            self.db = Neo4jInterface(\n",
    "                self.neo4j_uri,\n",
    "                self.neo4j_user,\n",
    "                self.neo4j_password\n",
    "            )\n",
    "            \n",
    "            # Initialize text processor\n",
    "            self.text_processor = TextProcessor()\n",
    "            \n",
    "            # Initialize LLM interface\n",
    "            self.llm = LLMInterface(\n",
    "                Config.OLLAMA_API,\n",
    "                Config.OLLAMA_MODEL\n",
    "            )\n",
    "            \n",
    "            # Verify environment\n",
    "            if not verify_environment():\n",
    "                raise RuntimeError(\"Environment verification failed\")\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Initialization failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _group_categories(self, categories: Dict[str, Category]) -> Dict[str, Set[str]]:\n",
    "        \"\"\"Group similar categories to optimize matching\"\"\"\n",
    "        category_groups = {}\n",
    "        \n",
    "        for cat_name, category in categories.items():\n",
    "            keywords = set(category.keywords)\n",
    "            \n",
    "            # Find groups with overlapping keywords\n",
    "            matching_groups = [\n",
    "                group_id for group_id, group_keywords in category_groups.items()\n",
    "                if len(keywords & group_keywords) / len(keywords) > 0.3\n",
    "            ]\n",
    "            \n",
    "            if matching_groups:\n",
    "                # Add to existing group with most overlap\n",
    "                best_group = max(\n",
    "                    matching_groups,\n",
    "                    key=lambda g: len(keywords & category_groups[g])\n",
    "                )\n",
    "                category_groups[best_group].update(keywords)\n",
    "            else:\n",
    "                # Create new group\n",
    "                group_id = f\"group_{len(category_groups)}\"\n",
    "                category_groups[group_id] = keywords\n",
    "        \n",
    "        return category_groups\n",
    "\n",
    "    async def process(self) -> Dict:\n",
    "        \"\"\"Main processing pipeline\"\"\"\n",
    "        if not self.initialize():\n",
    "            return {'error': 'Initialization failed'}\n",
    "        \n",
    "        try:\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # Get use cases\n",
    "            all_use_cases = list(self.db.use_cases.values())\n",
    "            \n",
    "            # Apply sample size if specified\n",
    "            if self.sample_size and self.sample_size < len(all_use_cases):\n",
    "                use_cases = all_use_cases[:self.sample_size]\n",
    "                self.logger.info(f\"Processing sample of {len(use_cases)} use cases\")\n",
    "            else:\n",
    "                use_cases = all_use_cases\n",
    "                self.logger.info(f\"Processing ALL {len(use_cases)} use cases\")\n",
    "            \n",
    "            # Group categories for optimization\n",
    "            self._group_categories(self.db.categories)\n",
    "            \n",
    "            # Process use cases\n",
    "            results = []\n",
    "            for use_case in tqdm(use_cases, desc=\"Processing Use Cases\"):\n",
    "                case_results = await self._process_use_case(use_case)\n",
    "                results.extend(case_results)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            self.stats = self._calculate_statistics(results)\n",
    "            \n",
    "            # Save results\n",
    "            if self.save_intermediates:\n",
    "                self._save_detailed_results(results)\n",
    "            \n",
    "            # Generate summary report\n",
    "            self.generate_summary_report()\n",
    "            \n",
    "            return self.stats\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Processing failed: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "        \n",
    "    async def _process_use_case(self, use_case: UseCase) -> List[MatchResult]:\n",
    "        \"\"\"Process a single use case\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for category in self.db.categories.values():\n",
    "            try:\n",
    "                # Perform matching\n",
    "                match_result = self.text_processor.evaluate_match(\n",
    "                    use_case, \n",
    "                    category\n",
    "                )\n",
    "                \n",
    "                # If no strong match found, try LLM\n",
    "                if match_result.method == MatchMethod.NO_MATCH and match_result.score > Config.Thresholds.RELATED:\n",
    "                    try:\n",
    "                        llm_result = await self.llm.evaluate_match(\n",
    "                            use_case,\n",
    "                            category,\n",
    "                            {\n",
    "                                'keyword_score': match_result.score,\n",
    "                                'semantic_score': match_result.score,\n",
    "                                'matched_terms': match_result.matched_terms or []\n",
    "                            }\n",
    "                        )\n",
    "                        \n",
    "                        if llm_result.is_match:\n",
    "                            llm_match = MatchResult(\n",
    "                                use_case_name=use_case.name,\n",
    "                                agency=use_case.agency,\n",
    "                                abbreviation=use_case.abbreviation or '',\n",
    "                                category_name=category.name,\n",
    "                                score=llm_result.confidence,\n",
    "                                method=MatchMethod.LLM,\n",
    "                                relationship_type=self.text_processor._get_relationship_type(llm_result.confidence),\n",
    "                                confidence=llm_result.confidence,\n",
    "                                justification=llm_result.justification\n",
    "                            )\n",
    "                            results.append(llm_match)\n",
    "                            self.db.save_match(llm_match)\n",
    "                    except Exception as llm_error:\n",
    "                        self.logger.error(f\"LLM evaluation error: {str(llm_error)}\")\n",
    "                \n",
    "                # Save initial match if significant\n",
    "                if match_result.method != MatchMethod.NO_MATCH:\n",
    "                    results.append(match_result)\n",
    "                    self.db.save_match(match_result)\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Log any errors during processing\n",
    "                self.logger.error(f\"Error processing use case {use_case.name} for category {category.name}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "               \n",
    "    \n",
    "    def _calculate_statistics(self, results: List[MatchResult]) -> Dict:\n",
    "        \"\"\"Calculate processing statistics\"\"\"\n",
    "        method_distribution = {\n",
    "            method.name: len([r for r in results if r.method == method])\n",
    "            for method in MatchMethod\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'total_processed': len(results),\n",
    "            'method_distribution': method_distribution,\n",
    "            'total_duration': time.time() - self.start_time,\n",
    "            'success_rate': len(results) / len(self.db.use_cases)\n",
    "        }\n",
    "    \n",
    "    def _save_detailed_results(self, results: List[MatchResult]):\n",
    "        \"\"\"Save detailed results to a timestamped CSV\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = self.output_dir / f\"technology_mapping_results_{timestamp}.csv\"\n",
    "        \n",
    "        # Use the existing save_match method to write results\n",
    "        with open(output_file, 'w', newline='') as csvfile:\n",
    "            for result in results:\n",
    "                self.db.save_match(result)\n",
    "        \n",
    "        self.logger.info(f\"Detailed results saved to {output_file}\")\n",
    "\n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
    "        report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_use_cases': len(self.db.use_cases),\n",
    "            'processed_use_cases': self.stats.get('total_processed', 0),\n",
    "            'processing_duration': self.stats.get('total_duration', 0),\n",
    "            'method_distribution': self.stats.get('method_distribution', {}),\n",
    "            'success_rate': self.stats.get('success_rate', 0),\n",
    "        }\n",
    "        \n",
    "        summary_file = self.output_dir / f\"mapping_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Async runner function\n",
    "async def run_technology_mapping(sample_size: Optional[int] = None):\n",
    "    \"\"\"Run the technology mapping process\"\"\"\n",
    "    try:\n",
    "        # Initialize and run the processor\n",
    "        processor = TechnologyMappingProcessor(\n",
    "            sample_size=sample_size,\n",
    "            save_intermediates=True\n",
    "        )\n",
    "        \n",
    "        # Run the mapping process\n",
    "        stats = await processor.process()\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\n--- Technology Mapping Results ---\")\n",
    "        print(json.dumps(stats, indent=2))\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Technology mapping failed: {str(e)}\")\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='technology_mapping.log'\n",
    ")\n",
    "\n",
    "# Execution for Jupyter notebook\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the mapping with sample size from Config\n",
    "    results = await run_technology_mapping(sample_size=Config.SAMPLE_SIZE)\n",
    "\n",
    "# For direct execution in Jupyter\n",
    "await run_technology_mapping(sample_size=Config.SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bff9aa-773f-4cd9-b3e7-18ba5cc3b4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j",
   "language": "python",
   "name": "neo4j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
