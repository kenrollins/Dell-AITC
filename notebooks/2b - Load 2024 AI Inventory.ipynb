{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa32934a-18cf-4ce2-8cf4-ca9cb4d4381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Federal AI Inventory Loading Notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"kuxFc8HN\"  # Update with your password\n",
    "CSV_PATH = r\"D:\\Docker\\neo4j\\import\\2024_Fed_AI_Inventory_condensed_v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b854cc72-0969-4377-9db2-f284ebebe95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1: Verify Python Dependencies\n",
    "def verify_dependencies():\n",
    "    required_packages = {\n",
    "        'neo4j': 'neo4j',\n",
    "        'pandas': 'pandas',\n",
    "        'numpy': 'numpy'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    for package, import_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(import_name)\n",
    "            print(f\"✓ {package} is installed\")\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "            print(f\"✗ {package} is missing\")\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"\\nPlease install missing packages using:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558351a3-0839-468a-b685-f84e35b00c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2: Verify Neo4j Connection and AI Categories\n",
    "def verify_neo4j_state(uri: str, user: str, password: str) -> bool:\n",
    "   try:\n",
    "       driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "       with driver.session() as session:\n",
    "           result = session.run(\"RETURN 'Connection test' as test\")\n",
    "           if result.single()['test'] != 'Connection test':\n",
    "               print(\"✗ Neo4j connection failed\")\n",
    "               return False\n",
    "           print(\"✓ Neo4j connection successful\")\n",
    "\n",
    "       with driver.session() as session:\n",
    "           result = session.run(\"\"\"\n",
    "               MATCH (c:AICategory)\n",
    "               RETURN count(c) as category_count\n",
    "           \"\"\")\n",
    "           category_count = result.single()['category_count']\n",
    "           if category_count == 0:\n",
    "              print(\"✗ No AI Categories found - please load categories first\")\n",
    "              return False\n",
    "           print(f\"✓ Found {category_count} AI Categories\")\n",
    "\n",
    "       with driver.session() as session:\n",
    "           result = session.run(\"\"\"\n",
    "               MATCH (u:UseCase)\n",
    "               RETURN count(u) as usecase_count\n",
    "           \"\"\")\n",
    "           usecase_count = result.single()['usecase_count']\n",
    "           if usecase_count > 0:\n",
    "               print(f\"! Warning: Found {usecase_count} existing Use Cases\")\n",
    "           else:\n",
    "               print(\"✓ No existing Use Cases found\")\n",
    "\n",
    "       with driver.session() as session:  \n",
    "           result = session.run(\"MATCH (m:Metadata) RETURN count(m) as metadata_count\")\n",
    "           metadata_count = result.single()['metadata_count'] \n",
    "           print(f\"✓ Found {metadata_count} Metadata nodes\")\n",
    "\n",
    "       with driver.session() as session:\n",
    "           result = session.run(\"MATCH (v:Version) RETURN count(v) as version_count\")\n",
    "           version_count = result.single()['version_count']\n",
    "           print(f\"✓ Found {version_count} Version nodes\")\n",
    "\n",
    "       driver.close()\n",
    "       return True\n",
    "\n",
    "   except Exception as e:\n",
    "       print(f\"✗ Neo4j verification failed: {str(e)}\")\n",
    "       return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1555b3b6-d2ab-4396-be08-51f9708dbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3: Verify CSV Data\n",
    "def verify_csv_data(csv_path: str) -> bool:\n",
    "    try:\n",
    "        # Check file exists\n",
    "        if not os.path.exists(csv_path):\n",
    "            print(f\"✗ CSV file not found: {csv_path}\")\n",
    "            return False\n",
    "        print(f\"✓ CSV file found: {csv_path}\")\n",
    "        \n",
    "        # Load and check structure\n",
    "        df = pd.read_csv(csv_path)\n",
    "        required_columns = [\n",
    "            'Use Case Name', 'Agency', 'Abr', 'Bureau', 'Topic Area',\n",
    "            'Dev Stage', 'Purpose Benefits', 'Outputs', 'System Name'\n",
    "        ]\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"✗ Missing required columns: {missing_columns}\")\n",
    "            return False\n",
    "        print(\"✓ All required columns present\")\n",
    "        \n",
    "        # Basic data quality checks\n",
    "        print(\"\\nData Quality Summary:\")\n",
    "        print(f\"- Total records: {len(df)}\")\n",
    "        print(f\"- Unique agencies: {df['Agency'].nunique()}\")\n",
    "        print(f\"- Unique use cases: {df['Use Case Name'].nunique()}\")\n",
    "        print(f\"- Records missing agency: {df['Agency'].isna().sum()}\")\n",
    "        print(f\"- Records missing use case name: {df['Use Case Name'].isna().sum()}\")\n",
    "        \n",
    "        # Check for duplicate use case names\n",
    "        duplicates = df['Use Case Name'].value_counts()\n",
    "        duplicates = duplicates[duplicates > 1]\n",
    "        if not duplicates.empty:\n",
    "            print(f\"\\n! Warning: Found {len(duplicates)} duplicate use case names:\")\n",
    "            for name, count in duplicates.items():\n",
    "                print(f\"  - '{name}': {count} occurrences\")\n",
    "                \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ CSV verification failed: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b8226b-1dd5-48ab-9d81-ea0493812f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4: Run All Verifications\n",
    "def run_verifications(neo4j_uri: str, neo4j_user: str, neo4j_password: str, csv_path: str) -> bool:\n",
    "    print(\"Starting verification checks...\\n\")\n",
    "    \n",
    "    print(\"1. Checking Python Dependencies:\")\n",
    "    if not verify_dependencies():\n",
    "        return False\n",
    "    print()\n",
    "    \n",
    "    print(\"2. Checking Neo4j State:\")\n",
    "    if not verify_neo4j_state(neo4j_uri, neo4j_user, neo4j_password):\n",
    "        return False\n",
    "    print()\n",
    "    \n",
    "    print(\"3. Checking CSV Data:\")\n",
    "    if not verify_csv_data(csv_path):\n",
    "        return False\n",
    "    print()\n",
    "    \n",
    "    print(\"✓ All verifications passed!\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0b7613-8a91-4ed1-a748-52c87a511865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting verification checks...\n",
      "\n",
      "1. Checking Python Dependencies:\n",
      "✓ neo4j is installed\n",
      "✓ pandas is installed\n",
      "✓ numpy is installed\n",
      "\n",
      "2. Checking Neo4j State:\n",
      "✓ Neo4j connection successful\n",
      "✓ Found 14 AI Categories\n",
      "✓ No existing Use Cases found\n",
      "✓ Found 1 Metadata nodes\n",
      "✓ Found 1 Version nodes\n",
      "\n",
      "3. Checking CSV Data:\n",
      "✓ CSV file found: D:\\Docker\\neo4j\\import\\2024_Fed_AI_Inventory_condensed_v2.csv\n",
      "✓ All required columns present\n",
      "\n",
      "Data Quality Summary:\n",
      "- Total records: 2133\n",
      "- Unique agencies: 42\n",
      "- Unique use cases: 2044\n",
      "- Records missing agency: 0\n",
      "- Records missing use case name: 0\n",
      "\n",
      "! Warning: Found 35 duplicate use case names:\n",
      "  - 'Generative AI Usage': 51 occurrences\n",
      "  - 'Veritone': 4 occurrences\n",
      "  - 'Chatbot': 3 occurrences\n",
      "  - 'ArcGIS': 3 occurrences\n",
      "  - 'Evidence.com - Axon': 3 occurrences\n",
      "  - 'Website Chatbot Assistant': 2 occurrences\n",
      "  - 'Automating blood smear cell counts using machine learning': 2 occurrences\n",
      "  - 'Machine learning for tsunami source zones': 2 occurrences\n",
      "  - 'Predicting inundation dynamics of small forested wetlands': 2 occurrences\n",
      "  - 'Grammarly': 2 occurrences\n",
      "  - 'Inventorying landforms with convolutional neural networks': 2 occurrences\n",
      "  - 'Data-driven approaches to filling missing time-series data within the San Francisco Bay-Delta': 2 occurrences\n",
      "  - 'AI Course Design Assistant': 2 occurrences\n",
      "  - 'Malware Reverse Engineering': 2 occurrences\n",
      "  - 'Improved earthquake detection for research studies': 2 occurrences\n",
      "  - 'Security Information and Event Management (SIEM) Alerting Models': 2 occurrences\n",
      "  - 'Image Editor': 2 occurrences\n",
      "  - 'Language Translation': 2 occurrences\n",
      "  - 'Microsoft Copilot Integration': 2 occurrences\n",
      "  - 'LexisNexis': 2 occurrences\n",
      "  - 'Bank Exam Quality Control': 2 occurrences\n",
      "  - 'National Training Team | Schools — FAQ Dashboard': 2 occurrences\n",
      "  - 'National Wildlife Disease Database': 2 occurrences\n",
      "  - 'Advanced Threat Protection': 2 occurrences\n",
      "  - 'Machine Learning Applied to Geotechnical Engineering: Statistical Methods Applied to Seismic Analysis 1': 2 occurrences\n",
      "  - 'ServiceNow': 2 occurrences\n",
      "  - 'Thomson Reuters CLEAR': 2 occurrences\n",
      "  - 'Salesforce': 2 occurrences\n",
      "  - 'Clearview AI': 2 occurrences\n",
      "  - 'Classifying GPS data to understand flight behavior of birds.': 2 occurrences\n",
      "  - 'Dragon': 2 occurrences\n",
      "  - 'LexisNexis (AI assisted legal research)': 2 occurrences\n",
      "  - 'Westlaw (AI assisted legal research)': 2 occurrences\n",
      "  - 'Prisoner Assessment Tool Targeting Estimated Risk and Needs (PATTERN)': 2 occurrences\n",
      "  - 'Zero shot segmentation to expedite Quaternary geologic mapping': 2 occurrences\n",
      "\n",
      "✓ All verifications passed!\n",
      "\n",
      "Ready to proceed with data loading!\n"
     ]
    }
   ],
   "source": [
    "# Run verifications before proceeding\n",
    "import os\n",
    "if run_verifications(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, CSV_PATH):\n",
    "    print(\"\\nReady to proceed with data loading!\")\n",
    "else:\n",
    "    print(\"\\n✗ Please resolve verification issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cda203-c811-4667-8dbb-bce0bd106fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 09:05:50,844 - INFO - Loaded 2133 records from CSV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting verification checks...\n",
      "\n",
      "1. Checking Python Dependencies:\n",
      "✓ neo4j is installed\n",
      "✓ pandas is installed\n",
      "✓ numpy is installed\n",
      "\n",
      "2. Checking Neo4j State:\n",
      "✓ Neo4j connection successful\n",
      "✓ Found 14 AI Categories\n",
      "✓ No existing Use Cases found\n",
      "✓ Found 1 Metadata nodes\n",
      "✓ Found 1 Version nodes\n",
      "\n",
      "3. Checking CSV Data:\n",
      "✓ CSV file found: D:\\Docker\\neo4j\\import\\2024_Fed_AI_Inventory_condensed_v2.csv\n",
      "✓ All required columns present\n",
      "\n",
      "Data Quality Summary:\n",
      "- Total records: 2133\n",
      "- Unique agencies: 42\n",
      "- Unique use cases: 2044\n",
      "- Records missing agency: 0\n",
      "- Records missing use case name: 0\n",
      "\n",
      "! Warning: Found 35 duplicate use case names:\n",
      "  - 'Generative AI Usage': 51 occurrences\n",
      "  - 'Veritone': 4 occurrences\n",
      "  - 'Chatbot': 3 occurrences\n",
      "  - 'ArcGIS': 3 occurrences\n",
      "  - 'Evidence.com - Axon': 3 occurrences\n",
      "  - 'Website Chatbot Assistant': 2 occurrences\n",
      "  - 'Automating blood smear cell counts using machine learning': 2 occurrences\n",
      "  - 'Machine learning for tsunami source zones': 2 occurrences\n",
      "  - 'Predicting inundation dynamics of small forested wetlands': 2 occurrences\n",
      "  - 'Grammarly': 2 occurrences\n",
      "  - 'Inventorying landforms with convolutional neural networks': 2 occurrences\n",
      "  - 'Data-driven approaches to filling missing time-series data within the San Francisco Bay-Delta': 2 occurrences\n",
      "  - 'AI Course Design Assistant': 2 occurrences\n",
      "  - 'Malware Reverse Engineering': 2 occurrences\n",
      "  - 'Improved earthquake detection for research studies': 2 occurrences\n",
      "  - 'Security Information and Event Management (SIEM) Alerting Models': 2 occurrences\n",
      "  - 'Image Editor': 2 occurrences\n",
      "  - 'Language Translation': 2 occurrences\n",
      "  - 'Microsoft Copilot Integration': 2 occurrences\n",
      "  - 'LexisNexis': 2 occurrences\n",
      "  - 'Bank Exam Quality Control': 2 occurrences\n",
      "  - 'National Training Team | Schools — FAQ Dashboard': 2 occurrences\n",
      "  - 'National Wildlife Disease Database': 2 occurrences\n",
      "  - 'Advanced Threat Protection': 2 occurrences\n",
      "  - 'Machine Learning Applied to Geotechnical Engineering: Statistical Methods Applied to Seismic Analysis 1': 2 occurrences\n",
      "  - 'ServiceNow': 2 occurrences\n",
      "  - 'Thomson Reuters CLEAR': 2 occurrences\n",
      "  - 'Salesforce': 2 occurrences\n",
      "  - 'Clearview AI': 2 occurrences\n",
      "  - 'Classifying GPS data to understand flight behavior of birds.': 2 occurrences\n",
      "  - 'Dragon': 2 occurrences\n",
      "  - 'LexisNexis (AI assisted legal research)': 2 occurrences\n",
      "  - 'Westlaw (AI assisted legal research)': 2 occurrences\n",
      "  - 'Prisoner Assessment Tool Targeting Estimated Risk and Needs (PATTERN)': 2 occurrences\n",
      "  - 'Zero shot segmentation to expedite Quaternary geologic mapping': 2 occurrences\n",
      "\n",
      "✓ All verifications passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 09:05:51,062 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT agency_name IF NOT EXISTS FOR (e:Agency) REQUIRE (e.name) IS UNIQUE` has no effect.} {description: `CONSTRAINT agency_name FOR (e:Agency) REQUIRE (e.name) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT agency_name IF NOT EXISTS FOR (a:Agency) REQUIRE a.name IS UNIQUE'\n",
      "2025-02-05 09:05:51,077 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT bureau_name IF NOT EXISTS FOR (e:Bureau) REQUIRE (e.name) IS UNIQUE` has no effect.} {description: `CONSTRAINT bureau_name FOR (e:Bureau) REQUIRE (e.name) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT bureau_name IF NOT EXISTS FOR (b:Bureau) REQUIRE b.name IS UNIQUE'\n",
      "2025-02-05 09:05:51,094 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE CONSTRAINT use_case_composite IF NOT EXISTS FOR (e:UseCase) REQUIRE (e.name, e.agency) IS UNIQUE` has no effect.} {description: `CONSTRAINT use_case_composite FOR (e:UseCase) REQUIRE (e.name, e.agency) IS UNIQUE` already exists.} {position: None} for query: 'CREATE CONSTRAINT use_case_composite IF NOT EXISTS FOR (u:UseCase) REQUIRE (u.name, u.agency) IS UNIQUE'\n",
      "2025-02-05 09:05:51,444 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX use_case_topic IF NOT EXISTS FOR (e:UseCase) ON (e.topic_area)` has no effect.} {description: `RANGE INDEX use_case_topic FOR (e:UseCase) ON (e.topic_area)` already exists.} {position: None} for query: 'CREATE INDEX use_case_topic IF NOT EXISTS FOR (u:UseCase) ON (u.topic_area)'\n",
      "2025-02-05 09:05:51,711 - INFO - Loaded metadata and version\n",
      "2025-02-05 09:05:51,993 - INFO - Loaded 42 agencies\n",
      "2025-02-05 09:05:52,373 - INFO - Loaded 336 bureaus\n",
      "2025-02-05 09:05:52,697 - INFO - Loaded use cases batch 1\n",
      "2025-02-05 09:05:52,780 - INFO - Loaded use cases batch 2\n",
      "2025-02-05 09:05:52,833 - INFO - Loaded use cases batch 3\n",
      "2025-02-05 09:05:52,937 - INFO - Loaded use cases batch 4\n",
      "2025-02-05 09:05:53,034 - INFO - Loaded use cases batch 5\n",
      "2025-02-05 09:05:53,128 - INFO - Loaded use cases batch 6\n",
      "2025-02-05 09:05:53,222 - INFO - Loaded use cases batch 7\n",
      "2025-02-05 09:05:53,261 - INFO - Loaded use cases batch 8\n",
      "2025-02-05 09:05:53,295 - INFO - Loaded use cases batch 9\n",
      "2025-02-05 09:05:53,384 - INFO - Loaded use cases batch 10\n",
      "2025-02-05 09:05:53,438 - INFO - Loaded use cases batch 11\n",
      "2025-02-05 09:05:53,518 - INFO - Loaded use cases batch 12\n",
      "2025-02-05 09:05:53,598 - INFO - Loaded use cases batch 13\n",
      "2025-02-05 09:05:53,679 - INFO - Loaded use cases batch 14\n",
      "2025-02-05 09:05:53,764 - INFO - Loaded use cases batch 15\n",
      "2025-02-05 09:05:53,803 - INFO - Loaded use cases batch 16\n",
      "2025-02-05 09:05:53,837 - INFO - Loaded use cases batch 17\n",
      "2025-02-05 09:05:53,880 - INFO - Loaded use cases batch 18\n",
      "2025-02-05 09:05:53,970 - INFO - Loaded use cases batch 19\n",
      "2025-02-05 09:05:54,057 - INFO - Loaded use cases batch 20\n",
      "2025-02-05 09:05:54,138 - INFO - Loaded use cases batch 21\n",
      "2025-02-05 09:05:54,206 - INFO - Loaded use cases batch 22\n",
      "2025-02-05 09:05:54,429 - INFO - Loaded 779 system relationships\n",
      "2025-02-05 09:05:55,445 - INFO - Loaded 2222 purpose/benefit relationships\n",
      "2025-02-05 09:05:56,067 - INFO - Loaded 2127 output relationships\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verification Results:\n",
      "==================================================\n",
      "Use Cases: 2052\n",
      "Agencies: 42\n",
      "Systems: 322\n",
      "Purpose Benefits: 2094\n",
      "Outputs: 1709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 09:05:56,386 - INFO - Data loading completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Cases with Systems: 767\n",
      "\n",
      "Duplicate Use Cases (properly handled with different agencies):\n",
      "- Security Information and Event Management (SIEM) Alerting Models: 2 instances\n",
      "- Grammarly: 2 instances\n",
      "- Language Translation: 2 instances\n",
      "- Chatbot: 2 instances\n",
      "- Malware Reverse Engineering: 2 instances\n"
     ]
    }
   ],
   "source": [
    "# Enhanced logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Neo4jConnection:\n",
    "    \"\"\"Manages Neo4j database connection and basic operations\"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Close the driver connection\"\"\"\n",
    "        self.driver.close()\n",
    "        \n",
    "    def verify_connection(self) -> bool:\n",
    "        \"\"\"Test database connection\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 'Connection test' as test\")\n",
    "                return result.single()['test'] == 'Connection test'\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Connection failed: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def execute_query(self, query: str, parameters: dict = None) -> List[Dict]:\n",
    "        \"\"\"Execute a Cypher query and return results\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(query, parameters or {})\n",
    "                return [record.data() for record in result]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Query execution failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "class FedAIDataLoader:\n",
    "    \"\"\"Handles loading Federal AI Inventory data into Neo4j\"\"\"\n",
    "    \n",
    "    def __init__(self, connection: Neo4jConnection, csv_path: str):\n",
    "        self.connection = connection\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        \n",
    "    def load_csv(self) -> None:\n",
    "        \"\"\"Load and prepare CSV data\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.csv_path)\n",
    "            logger.info(f\"Loaded {len(self.df)} records from CSV\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load CSV: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def create_constraints(self) -> None:\n",
    "        \"\"\"Create database constraints and indexes\"\"\"\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT agency_name IF NOT EXISTS FOR (a:Agency) REQUIRE a.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT bureau_name IF NOT EXISTS FOR (b:Bureau) REQUIRE b.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT use_case_composite IF NOT EXISTS FOR (u:UseCase) REQUIRE (u.name, u.agency) IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT system_name IF NOT EXISTS FOR (s:System) REQUIRE s.name IS UNIQUE\",\n",
    "            \"CREATE INDEX use_case_topic IF NOT EXISTS FOR (u:UseCase) ON (u.topic_area)\"\n",
    "        ]\n",
    "        \n",
    "        for constraint in constraints:\n",
    "            try:\n",
    "                self.connection.execute_query(constraint)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Constraint creation warning: {str(e)}\")\n",
    "\n",
    "    def load_metadata(self) -> None:\n",
    "        query = \"\"\"\n",
    "        MERGE (m:Metadata {version: '1.0'})\n",
    "        MERGE (v:Version {number: '1.0'})\n",
    "        MERGE (m)-[:CURRENT_VERSION]->(v)\n",
    "        \"\"\"\n",
    "        self.connection.execute_query(query)\n",
    "        logger.info(\"Loaded metadata and version\")\n",
    "                \n",
    "    def load_agencies(self) -> None:\n",
    "        \"\"\"Load agencies and create relationships\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $agencies AS agency\n",
    "        MERGE (a:Agency {name: agency.name})\n",
    "        SET a.abbreviation = agency.abbreviation\n",
    "        \"\"\"\n",
    "        \n",
    "        agencies = self.df[['Agency', 'Abr']].drop_duplicates().to_dict('records')\n",
    "        agencies = [{'name': a['Agency'], 'abbreviation': a['Abr']} for a in agencies]\n",
    "        \n",
    "        self.connection.execute_query(query, {'agencies': agencies})\n",
    "        logger.info(f\"Loaded {len(agencies)} agencies\")\n",
    "        \n",
    "    def load_bureaus(self) -> None:\n",
    "        \"\"\"Load bureaus and link to agencies\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $bureaus AS bureau\n",
    "        MATCH (a:Agency {name: bureau.agency})\n",
    "        MERGE (b:Bureau {name: bureau.name})\n",
    "        MERGE (a)-[:HAS_BUREAU]->(b)\n",
    "        \"\"\"\n",
    "        \n",
    "        bureaus = self.df[['Agency', 'Bureau']].dropna().drop_duplicates().to_dict('records')\n",
    "        bureaus = [{'agency': b['Agency'], 'name': b['Bureau']} for b in bureaus]\n",
    "        \n",
    "        self.connection.execute_query(query, {'bureaus': bureaus})\n",
    "        logger.info(f\"Loaded {len(bureaus)} bureaus\")\n",
    "        \n",
    "    def load_use_cases(self) -> None:\n",
    "        \"\"\"Load use cases with properties using composite key (name + agency)\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $use_cases AS uc\n",
    "        MATCH (a:Agency {name: uc.agency})\n",
    "        MERGE (u:UseCase {name: uc.name, agency: uc.agency})\n",
    "        SET u.topic_area = uc.topic_area,\n",
    "            u.dev_stage = uc.dev_stage,\n",
    "            u.purpose_benefits = uc.purpose_benefits,\n",
    "            u.outputs = uc.outputs,\n",
    "            u.date_initiated = uc.date_initiated,\n",
    "            u.contains_pii = uc.contains_pii,\n",
    "            u.has_ato = uc.has_ato,\n",
    "            u.infrastructure = uc.infrastructure,\n",
    "            u.updated_at = datetime()\n",
    "        MERGE (a)-[:HAS_USE_CASE]->(u)\n",
    "        \"\"\"\n",
    "    \n",
    "        use_cases = [{\n",
    "            'agency': uc['Agency'],\n",
    "            'name': uc['Use Case Name'], \n",
    "            'topic_area': uc['Topic Area'],\n",
    "            'dev_stage': uc['Dev Stage'],\n",
    "            'purpose_benefits': uc['Purpose Benefits'],\n",
    "            'outputs': uc['Outputs'],\n",
    "            'date_initiated': uc['Date Initiated'],\n",
    "            'contains_pii': uc['Contains PII'] == 'Yes',\n",
    "            'has_ato': uc['Has ATO'] == 'Yes',\n",
    "            'infrastructure': uc['Infrastructure Provisioned']\n",
    "        } for uc in self.df.to_dict('records')]\n",
    "    \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(use_cases), batch_size):\n",
    "            batch = use_cases[i:i + batch_size]\n",
    "            self.connection.execute_query(query, {'use_cases': batch})\n",
    "            logger.info(f\"Loaded use cases batch {i//batch_size + 1}\")\n",
    "            \n",
    "    def load_systems(self) -> None:\n",
    "        \"\"\"Load systems and create relationships\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $systems AS sys\n",
    "        MATCH (u:UseCase {name: sys.use_case, agency: sys.agency})\n",
    "        MERGE (s:System {name: sys.name})\n",
    "        MERGE (u)-[:USES_SYSTEM]->(s)\n",
    "        \"\"\"\n",
    "        \n",
    "        systems = self.df[['Use Case Name', 'Agency', 'System Name']].dropna().to_dict('records')\n",
    "        systems = [{\n",
    "            'use_case': s['Use Case Name'],\n",
    "            'agency': s['Agency'],\n",
    "            'name': s['System Name']\n",
    "        } for s in systems]\n",
    "        \n",
    "        self.connection.execute_query(query, {'systems': systems})\n",
    "        logger.info(f\"Loaded {len(systems)} system relationships\")\n",
    "        \n",
    "    def load_purpose_benefits(self) -> None:\n",
    "        \"\"\"Parse and load purpose/benefits\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $purposes AS p\n",
    "        MATCH (u:UseCase {name: p.use_case, agency: p.agency})\n",
    "        MERGE (pb:PurposeBenefit {description: p.description})\n",
    "        MERGE (u)-[:HAS_PURPOSE]->(pb)\n",
    "        \"\"\"\n",
    "        \n",
    "        def parse_purposes(text):\n",
    "            if pd.isna(text):\n",
    "                return []\n",
    "            return [p.strip() for p in text.split(';') if p.strip()]\n",
    "        \n",
    "        purposes = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            for purpose in parse_purposes(row['Purpose Benefits']):\n",
    "                purposes.append({\n",
    "                    'use_case': row['Use Case Name'],\n",
    "                    'agency': row['Agency'],\n",
    "                    'description': purpose\n",
    "                })\n",
    "        \n",
    "        self.connection.execute_query(query, {'purposes': purposes})\n",
    "        logger.info(f\"Loaded {len(purposes)} purpose/benefit relationships\")\n",
    "        \n",
    "    def load_outputs(self) -> None:\n",
    "        \"\"\"Parse and load outputs\"\"\"\n",
    "        query = \"\"\"\n",
    "        UNWIND $outputs AS out\n",
    "        MATCH (u:UseCase {name: out.use_case, agency: out.agency})\n",
    "        MERGE (o:Output {description: out.description})\n",
    "        MERGE (u)-[:PRODUCES]->(o)\n",
    "        \"\"\"\n",
    "        \n",
    "        def parse_outputs(text):\n",
    "            if pd.isna(text):\n",
    "                return []\n",
    "            return [o.strip() for o in text.split(';') if o.strip()]\n",
    "        \n",
    "        outputs = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            for output in parse_outputs(row['Outputs']):\n",
    "                outputs.append({\n",
    "                    'use_case': row['Use Case Name'],\n",
    "                    'agency': row['Agency'],\n",
    "                    'description': output\n",
    "                })\n",
    "        \n",
    "        self.connection.execute_query(query, {'outputs': outputs})\n",
    "        logger.info(f\"Loaded {len(outputs)} output relationships\")\n",
    "\n",
    "def verify_loading(connection: Neo4jConnection) -> None:\n",
    "    \"\"\"Verify the data loading results\"\"\"\n",
    "    verification_queries = {\n",
    "        \"Use Cases\": \"MATCH (u:UseCase) RETURN count(u) as count\",\n",
    "        \"Agencies\": \"MATCH (a:Agency) RETURN count(a) as count\",\n",
    "        \"Systems\": \"MATCH (s:System) RETURN count(s) as count\",\n",
    "        \"Purpose Benefits\": \"MATCH (p:PurposeBenefit) RETURN count(p) as count\",\n",
    "        \"Outputs\": \"MATCH (o:Output) RETURN count(o) as count\",\n",
    "        \"Use Cases with Systems\": \"\"\"\n",
    "            MATCH (u:UseCase)-[:USES_SYSTEM]->()\n",
    "            RETURN count(DISTINCT u) as count\n",
    "        \"\"\",\n",
    "        \"Duplicate Check\": \"\"\"\n",
    "            MATCH (u:UseCase)\n",
    "            WITH u.name as name, count(*) as instances\n",
    "            WHERE instances > 1\n",
    "            RETURN name, instances\n",
    "            ORDER BY instances DESC\n",
    "            LIMIT 5\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    print(\"\\nVerification Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for description, query in verification_queries.items():\n",
    "        try:\n",
    "            result = connection.execute_query(query)\n",
    "            if description != \"Duplicate Check\":\n",
    "                print(f\"{description}: {result[0]['count']}\")\n",
    "            else:\n",
    "                if result:\n",
    "                    print(f\"\\nDuplicate Use Cases (properly handled with different agencies):\")\n",
    "                    for record in result:\n",
    "                        print(f\"- {record['name']}: {record['instances']} instances\")\n",
    "                else:\n",
    "                    print(\"\\nNo duplicates found\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Verification failed for {description}: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    # Run initial verifications\n",
    "    if not run_verifications(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD, CSV_PATH):\n",
    "        logger.error(\"Verification failed. Please resolve issues before proceeding.\")\n",
    "        return\n",
    "        \n",
    "    # Initialize connection\n",
    "    connection = Neo4jConnection(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "    \n",
    "    try:\n",
    "        # Initialize loader\n",
    "        loader = FedAIDataLoader(connection, CSV_PATH)\n",
    "        \n",
    "        # Execute loading pipeline\n",
    "        loader.load_csv()\n",
    "        loader.create_constraints()\n",
    "        \n",
    "        # Load entities and relationships\n",
    "        loader.load_metadata()\n",
    "        loader.load_agencies()\n",
    "        loader.load_bureaus()\n",
    "        loader.load_use_cases()\n",
    "        loader.load_systems()\n",
    "        loader.load_purpose_benefits()\n",
    "        loader.load_outputs()\n",
    "        \n",
    "        # Verify loading\n",
    "        verify_loading(connection)\n",
    "        \n",
    "        logger.info(\"Data loading completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Loading failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
