{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90868619-c25b-4492-8a58-e74dcd4fb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Technology Categories Import Notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from neo4j import GraphDatabase\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import re\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "NEO4J_URI = \"neo4j://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"kuxFc8HN\"  # Adjust as needed\n",
    "CSV_PATH = r\"D:\\Docker\\neo4j\\import\\AI-Technology-Categories-v1.3.csv\"  # Updated to v1.3\n",
    "\n",
    "# Enhanced logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def verify_neo4j_connection() -> bool:\n",
    "    \"\"\"Verify Neo4j connection with enhanced error reporting\"\"\"\n",
    "    try:\n",
    "        with GraphDatabase.driver(\n",
    "            NEO4J_URI, \n",
    "            auth=(NEO4J_USER, NEO4J_PASSWORD),\n",
    "            connection_timeout=5\n",
    "        ) as driver:\n",
    "            driver.verify_connectivity()\n",
    "            logger.info(\"Neo4j connection verified successfully\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Neo4j connection failed: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab829b42-f459-46c2-9fee-3e8bd02283e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITechDataPrep:\n",
    "    \"\"\"Handles data preparation and validation for AI Technology Categories\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def load_and_prepare(self) -> pd.DataFrame:\n",
    "        \"\"\"Primary data loading and preparation pipeline\"\"\"\n",
    "        try:\n",
    "            # Read CSV file\n",
    "            self.df = pd.read_csv(self.csv_path, encoding='utf-8')\n",
    "            \n",
    "            # Print actual columns for debugging\n",
    "            self.logger.info(\"Actual columns in file:\")\n",
    "            self.logger.info(self.df.columns.tolist())\n",
    "            \n",
    "            self._sanitize_data()\n",
    "            self._validate_schema()\n",
    "            self._prepare_relationships()\n",
    "            return self.df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Data preparation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _sanitize_data(self) -> None:\n",
    "        \"\"\"Sanitize data for Neo4j import\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"DataFrame not initialized\")\n",
    "            \n",
    "        self.df = self.df.replace({\n",
    "            r'\\n': ' ',\n",
    "            r'\\r': ' ',\n",
    "            r'\\t': ' ',\n",
    "            r'\\s+': ' '  # Collapse multiple spaces\n",
    "        }, regex=True)\n",
    "        \n",
    "        # Strip whitespace and handle null values\n",
    "        for column in self.df.columns:\n",
    "            if self.df[column].dtype == object:\n",
    "                self.df[column] = self.df[column].str.strip()\n",
    "                \n",
    "        self.logger.info(\"Data sanitization completed\")\n",
    "\n",
    "    def _validate_schema(self) -> None:\n",
    "        \"\"\"Validate required columns and data types\"\"\"\n",
    "        required_columns = {\n",
    "            'ai_category': str,\n",
    "            'category_definition': str,\n",
    "            'zone': str,\n",
    "            'keywords': str,\n",
    "            'capabilities': str,\n",
    "            'business_language': str,\n",
    "            'input_data_types': str,\n",
    "            'generated_output': str,\n",
    "            'operational_metrics': str,\n",
    "            'model_artifacts': str,\n",
    "            'vendor_ecosystem_tier': str,\n",
    "            'dependent_technologies': str,\n",
    "            'maturity_level': str,\n",
    "            'integration_patterns': str\n",
    "        }\n",
    "        \n",
    "        missing_columns = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "            \n",
    "        # Validate zone values\n",
    "        valid_zones = {\n",
    "            'Analytical Intelligence',\n",
    "            'Domain Specific',\n",
    "            'Enterprise Enablement',\n",
    "            'Core Infrastructure'\n",
    "        }\n",
    "        if self.df is not None and 'zone' in self.df.columns:\n",
    "            invalid_zones = set(self.df['zone'].unique()) - valid_zones\n",
    "            if invalid_zones:\n",
    "                raise ValueError(f\"Invalid zone values found: {invalid_zones}\")\n",
    "\n",
    "        self.logger.info(\"Schema validation completed\")\n",
    "\n",
    "    def _prepare_relationships(self) -> None:\n",
    "        \"\"\"Prepare relationship data for Neo4j import\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"DataFrame not initialized\")\n",
    "            \n",
    "        # Split delimited fields and create relationship mappings\n",
    "        relationship_columns = [\n",
    "            'keywords', 'capabilities', 'integration_patterns',\n",
    "            'vendor_ecosystem_tier', 'dependent_technologies'\n",
    "        ]\n",
    "        \n",
    "        for column in relationship_columns:\n",
    "            if column in self.df.columns:\n",
    "                # Create relationship lists\n",
    "                self.df[f'{column}_list'] = self.df[column].str.split(';')\n",
    "                \n",
    "                # Clean individual items\n",
    "                self.df[f'{column}_list'] = self.df[f'{column}_list'].apply(\n",
    "                    lambda x: [item.strip() for item in x] if isinstance(x, list) else []\n",
    "                )\n",
    "\n",
    "        self.logger.info(\"Relationship data preparation completed\")\n",
    "\n",
    "    def get_unique_values(self, column: str) -> List[str]:\n",
    "        \"\"\"Extract unique values from a semicolon-delimited column\"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"DataFrame not initialized\")\n",
    "            \n",
    "        if column not in self.df.columns:\n",
    "            raise ValueError(f\"Column {column} not found in DataFrame\")\n",
    "            \n",
    "        values = set()\n",
    "        for item in self.df[column].str.split(';'):\n",
    "            if isinstance(item, list):\n",
    "                values.update([x.strip() for x in item])\n",
    "        return sorted(list(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "884dc5f9-b380-403c-b181-8612473f5425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jTransactionManager:\n",
    "    \"\"\"Manages Neo4j transactions with retry logic and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, uri: str, user: str, password: str):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"Clean up driver resources\"\"\"\n",
    "        self.driver.close()\n",
    "\n",
    "    def execute_write(self, query: str, params: Dict = None, retries: int = 3) -> None:\n",
    "        \"\"\"Execute write operations with retry logic\"\"\"\n",
    "        last_error = None\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                with self.driver.session() as session:\n",
    "                    session.execute_write(\n",
    "                        lambda tx: tx.run(query, params or {})\n",
    "                    )\n",
    "                return\n",
    "            except Exception as e:\n",
    "                last_error = e\n",
    "                self.logger.warning(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt < retries - 1:\n",
    "                    continue\n",
    "                raise last_error\n",
    "\n",
    "    def execute_read(self, query: str, params: Dict = None) -> List[Dict]:\n",
    "        \"\"\"Execute read operations with error handling\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.execute_read(\n",
    "                    lambda tx: list(tx.run(query, params or {}))\n",
    "                )\n",
    "                return [record.data() for record in result]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Read operation failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def cleanup_database(self) -> None:\n",
    "        \"\"\"Emergency cleanup for failed imports\"\"\"\n",
    "        try:\n",
    "            self.execute_write(\"MATCH (n) DETACH DELETE n\")\n",
    "            self.logger.info(\"Database cleanup completed\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Cleanup failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_constraints(self) -> None:\n",
    "        \"\"\"Set up required constraints and indices\"\"\"\n",
    "        constraints = [\n",
    "            \"CREATE CONSTRAINT ai_category_id IF NOT EXISTS FOR (c:AICategory) REQUIRE c.id IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT keyword_name IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT capability_name IF NOT EXISTS FOR (c:Capability) REQUIRE c.name IS UNIQUE\",\n",
    "            \"CREATE CONSTRAINT zone_name IF NOT EXISTS FOR (z:Zone) REQUIRE z.name IS UNIQUE\",\n",
    "            \"CREATE INDEX ai_category_name IF NOT EXISTS FOR (c:AICategory) ON (c.name)\",\n",
    "            \"CREATE INDEX ai_category_zone IF NOT EXISTS FOR (c:AICategory) ON (c.zone)\",\n",
    "            \"CREATE INDEX keyword_index IF NOT EXISTS FOR (k:Keyword) ON (k.name)\",\n",
    "            \"CREATE INDEX capability_index IF NOT EXISTS FOR (c:Capability) ON (c.name)\"\n",
    "        ]\n",
    "        \n",
    "        for constraint in constraints:\n",
    "            try:\n",
    "                self.execute_write(constraint)\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Constraint creation failed: {str(e)}\")\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b1ed9c-6eea-4f08-84f8-658d85505313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITechImporter:\n",
    "    \"\"\"Orchestrates the import of AI Technology Categories into Neo4j\"\"\"\n",
    "    \n",
    "    def __init__(self, tx_manager: Neo4jTransactionManager):\n",
    "        self.tx_manager = tx_manager\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def import_categories(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Stage 1: Create core category nodes\"\"\"\n",
    "        # First create Zone nodes\n",
    "        zone_query = \"\"\"\n",
    "        UNWIND $zones as z\n",
    "        MERGE (zone:Zone {name: z})\n",
    "        \"\"\"\n",
    "        \n",
    "        unique_zones = df['zone'].unique().tolist()\n",
    "        self.tx_manager.execute_write(zone_query, {'zones': unique_zones})\n",
    "        self.logger.info(f\"Created zones: {', '.join(unique_zones)}\")\n",
    "\n",
    "        # Then create categories and link to zones\n",
    "        category_query = \"\"\"\n",
    "        MATCH (z:Zone {name: $zone})\n",
    "        MERGE (c:AICategory {id: $id})\n",
    "        SET c.name = $name,\n",
    "            c.category_definition = $category_definition,\n",
    "            c.created_at = datetime(),\n",
    "            c.maturity_level = $maturity_level,\n",
    "            c.zone = $zone\n",
    "        MERGE (c)-[:BELONGS_TO]->(z)\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            params = {\n",
    "                'id': str(uuid.uuid4()),\n",
    "                'name': row['ai_category'],\n",
    "                'category_definition': row['category_definition'],\n",
    "                'maturity_level': row['maturity_level'],\n",
    "                'zone': row['zone']\n",
    "            }\n",
    "            try:\n",
    "                self.tx_manager.execute_write(category_query, params)\n",
    "                self.logger.info(f\"Created category: {row['ai_category']} in zone: {row['zone']}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to create category {row['ai_category']}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "    def import_keywords_and_capabilities(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Stage 2: Create keyword and capability relationships\"\"\"\n",
    "        keyword_query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})\n",
    "        MERGE (k:Keyword {name: $keyword})\n",
    "        MERGE (c)-[r:TAGGED_WITH]->(k)\n",
    "        ON CREATE SET r.weight = 1\n",
    "        ON MATCH SET r.weight = r.weight + 1\n",
    "        \"\"\"\n",
    "        \n",
    "        capability_query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})\n",
    "        MERGE (cap:Capability {name: $capability})\n",
    "        MERGE (c)-[:HAS_CAPABILITY]->(cap)\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            category = row['ai_category']\n",
    "            \n",
    "            # Process keywords\n",
    "            keywords = row['keywords'].split(';')\n",
    "            for kw in keywords:\n",
    "                self.tx_manager.execute_write(keyword_query, {\n",
    "                    'category': category,\n",
    "                    'keyword': kw.strip()\n",
    "                })\n",
    "            \n",
    "            # Process capabilities\n",
    "            capabilities = row['capabilities'].split(';')\n",
    "            for cap in capabilities:\n",
    "                self.tx_manager.execute_write(capability_query, {\n",
    "                    'category': category,\n",
    "                    'capability': cap.strip()\n",
    "                })\n",
    "\n",
    "    def import_dependencies(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Import dependencies with case-insensitive matching and zone tracking\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c1:AICategory)-[:BELONGS_TO]->(z1:Zone)\n",
    "        WHERE toLower(c1.name) = toLower($source)\n",
    "        MATCH (c2:AICategory)-[:BELONGS_TO]->(z2:Zone)\n",
    "        WHERE toLower(c2.name) = toLower($target)\n",
    "        MERGE (c1)-[r:DEPENDS_ON]->(c2)\n",
    "        SET r.cross_zone = CASE WHEN z1.name = z2.name THEN false ELSE true END,\n",
    "            r.created_at = datetime()\n",
    "        \"\"\"\n",
    "    \n",
    "        for _, row in df.iterrows():\n",
    "            source = row['ai_category']\n",
    "            dependencies = row['dependent_technologies'].split(';')\n",
    "        \n",
    "            for dep in dependencies:\n",
    "                dep = dep.strip()\n",
    "                if dep:  # Skip empty strings\n",
    "                    try:\n",
    "                        self.tx_manager.execute_write(query, {\n",
    "                            'source': source,\n",
    "                            'target': dep\n",
    "                        })\n",
    "                        self.logger.info(f\"Created dependency: {source} -> {dep}\")\n",
    "                    except Exception as e:\n",
    "                        self.logger.error(f\"Failed to create dependency {source} -> {dep}: {str(e)}\")\n",
    "\n",
    "    def import_integration_patterns(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Stage 4: Create integration pattern relationships\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})\n",
    "        MERGE (i:IntegrationPattern {name: $pattern})\n",
    "        MERGE (c)-[:INTEGRATES_VIA]->(i)\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            category = row['ai_category']\n",
    "            patterns = row['integration_patterns'].split(';')\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                pattern = pattern.strip()\n",
    "                if pattern:\n",
    "                    self.tx_manager.execute_write(query, {\n",
    "                        'category': category,\n",
    "                        'pattern': pattern\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93913620-5225-4eb0-a4c6-79c89113a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class ImportVerification:\n",
    "    \"\"\"Verifies data import integrity and relationships\"\"\"\n",
    "    \n",
    "    def __init__(self, tx_manager: Neo4jTransactionManager):\n",
    "        self.tx_manager = tx_manager\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def verify_categories(self) -> Dict[str, Any]:\n",
    "        \"\"\"Verify core category import\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory)\n",
    "        RETURN count(c) as category_count,\n",
    "               count(DISTINCT c.id) as unique_ids,\n",
    "               count(c.category_definition) as definitions,\n",
    "               collect(DISTINCT c.maturity_level) as maturity_levels,\n",
    "               collect(DISTINCT c.zone) as zones\n",
    "        \"\"\"\n",
    "        \n",
    "        results = self.tx_manager.execute_read(query)[0]\n",
    "        return {\n",
    "            'status': 'PASS' if results['category_count'] == results['unique_ids'] else 'FAIL',\n",
    "            'counts': results\n",
    "        }\n",
    "\n",
    "    def verify_relationships(self) -> Dict[str, Any]:\n",
    "        \"\"\"Verify relationship integrity\"\"\"\n",
    "        queries = {\n",
    "            'keywords': \"\"\"\n",
    "                MATCH (c:AICategory)-[r:TAGGED_WITH]->(k:Keyword)\n",
    "                RETURN count(DISTINCT c) as categories,\n",
    "                       count(DISTINCT k) as keywords,\n",
    "                       count(r) as relationships\n",
    "            \"\"\",\n",
    "            'capabilities': \"\"\"\n",
    "                MATCH (c:AICategory)-[r:HAS_CAPABILITY]->(cap:Capability)\n",
    "                RETURN count(DISTINCT c) as categories,\n",
    "                       count(DISTINCT cap) as capabilities,\n",
    "                       count(r) as relationships\n",
    "            \"\"\",\n",
    "            'dependencies': \"\"\"\n",
    "                MATCH (c1:AICategory)-[r:DEPENDS_ON]->(c2:AICategory)\n",
    "                RETURN count(DISTINCT c1) as source_categories,\n",
    "                       count(DISTINCT c2) as target_categories,\n",
    "                       count(r) as relationships\n",
    "            \"\"\",\n",
    "            'zones': \"\"\"\n",
    "                MATCH (c:AICategory)-[:BELONGS_TO]->(z:Zone)\n",
    "                RETURN count(DISTINCT z) as zone_count,\n",
    "                       count(c) as categorized_count,\n",
    "                       collect(DISTINCT z.name) as zone_names\n",
    "            \"\"\"\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, query in queries.items():\n",
    "            results[name] = self.tx_manager.execute_read(query)[0]\n",
    "            \n",
    "        return {\n",
    "            'status': 'PASS' if self._check_relationship_integrity(results) else 'FAIL',\n",
    "            'counts': results\n",
    "        }\n",
    "\n",
    "    def _check_relationship_integrity(self, results: Dict) -> bool:\n",
    "        return (\n",
    "            results['keywords']['categories'] > 0 and\n",
    "            results['keywords']['keywords'] > 0 and\n",
    "            results['capabilities']['categories'] > 0 and\n",
    "            results['zones']['zone_count'] == 4  # We expect exactly 4 zones\n",
    "        )\n",
    "\n",
    "    def verify_data_consistency(self) -> Dict[str, Any]:\n",
    "        \"\"\"Verify data consistency and referential integrity\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory)\n",
    "        OPTIONAL MATCH (c)-[:TAGGED_WITH]->(k:Keyword)\n",
    "        OPTIONAL MATCH (c)-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "        OPTIONAL MATCH (c)-[:DEPENDS_ON]->(d:AICategory)\n",
    "        OPTIONAL MATCH (c)-[:BELONGS_TO]->(z:Zone)\n",
    "        RETURN c.name as category,\n",
    "               z.name as zone,\n",
    "               count(DISTINCT k) as keyword_count,\n",
    "               count(DISTINCT cap) as capability_count,\n",
    "               count(DISTINCT d) as dependency_count\n",
    "        \"\"\"\n",
    "        \n",
    "        results = self.tx_manager.execute_read(query)\n",
    "        \n",
    "        return {\n",
    "            'status': 'PASS' if self._validate_consistency(results) else 'FAIL',\n",
    "            'details': results\n",
    "        }\n",
    "\n",
    "    def _validate_consistency(self, results: List[Dict]) -> bool:\n",
    "        \"\"\"Check for minimum relationship requirements\"\"\"\n",
    "        return all(\n",
    "            r['keyword_count'] > 0 and \n",
    "            r['capability_count'] > 0 and\n",
    "            r['zone'] is not None\n",
    "            for r in results\n",
    "        )\n",
    "\n",
    "    def run_full_verification(self) -> Dict[str, Any]:\n",
    "        \"\"\"Execute all verification checks\"\"\"\n",
    "        verifications = {\n",
    "            'categories': self.verify_categories(),\n",
    "            'relationships': self.verify_relationships(),\n",
    "            'consistency': self.verify_data_consistency()\n",
    "        }\n",
    "        \n",
    "        overall_status = all(v['status'] == 'PASS' for v in verifications.values())\n",
    "        \n",
    "        return {\n",
    "            'status': 'PASS' if overall_status else 'FAIL',\n",
    "            'details': verifications\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f06ece-bd49-4ec2-8d4f-920d06200033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 19:04:19,592 - INFO - Neo4j connection verified successfully\n",
      "2025-02-02 19:04:19,599 - INFO - Actual columns in file:\n",
      "2025-02-02 19:04:19,600 - INFO - ['ai_category', 'category_definition', 'zone', 'keywords', 'capabilities', 'business_language', 'input_data_types', 'generated_output', 'operational_metrics', 'model_artifacts', 'vendor_ecosystem_tier', 'dependent_technologies', 'maturity_level', 'integration_patterns']\n",
      "2025-02-02 19:04:19,608 - INFO - Data sanitization completed\n",
      "2025-02-02 19:04:19,609 - INFO - Schema validation completed\n",
      "2025-02-02 19:04:19,613 - INFO - Relationship data preparation completed\n",
      "2025-02-02 19:04:20,143 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX keyword_index IF NOT EXISTS FOR (e:Keyword) ON (e.name)` has no effect.} {description: `RANGE INDEX keyword_name FOR (e:Keyword) ON (e.name)` already exists.} {position: None} for query: 'CREATE INDEX keyword_index IF NOT EXISTS FOR (k:Keyword) ON (k.name)'\n",
      "2025-02-02 19:04:20,159 - INFO - Received notification from DBMS server: {severity: INFORMATION} {code: Neo.ClientNotification.Schema.IndexOrConstraintAlreadyExists} {category: SCHEMA} {title: `CREATE RANGE INDEX capability_index IF NOT EXISTS FOR (e:Capability) ON (e.name)` has no effect.} {description: `RANGE INDEX capability_name FOR (e:Capability) ON (e.name)` already exists.} {position: None} for query: 'CREATE INDEX capability_index IF NOT EXISTS FOR (c:Capability) ON (c.name)'\n",
      "2025-02-02 19:04:20,161 - INFO - Importing categories...\n",
      "2025-02-02 19:04:20,307 - INFO - Created zones: Core Infrastructure, Analytical Intelligence, Enterprise Enablement, Domain Specific\n",
      "2025-02-02 19:04:20,471 - INFO - Created category: AI Development & Operations (AIOps) in zone: Core Infrastructure\n",
      "2025-02-02 19:04:20,555 - INFO - Created category: Computer Vision & Media Analysis in zone: Analytical Intelligence\n",
      "2025-02-02 19:04:20,573 - INFO - Created category: Cybersecurity & Threat Detection in zone: Core Infrastructure\n",
      "2025-02-02 19:04:20,590 - INFO - Created category: Data Integration & Management in zone: Core Infrastructure\n",
      "2025-02-02 19:04:20,608 - INFO - Created category: Decision Support & Optimization in zone: Enterprise Enablement\n",
      "2025-02-02 19:04:20,628 - INFO - Created category: Edge AI & IoT in zone: Core Infrastructure\n",
      "2025-02-02 19:04:20,644 - INFO - Created category: Environmental & Geospatial AI in zone: Domain Specific\n",
      "2025-02-02 19:04:20,662 - INFO - Created category: Healthcare & Biotech AI in zone: Domain Specific\n",
      "2025-02-02 19:04:20,679 - INFO - Created category: Intelligent End-User Computing in zone: Enterprise Enablement\n",
      "2025-02-02 19:04:20,697 - INFO - Created category: Multimodal AI Systems in zone: Analytical Intelligence\n",
      "2025-02-02 19:04:20,715 - INFO - Created category: Natural Language Processing (NLP) in zone: Analytical Intelligence\n",
      "2025-02-02 19:04:20,733 - INFO - Created category: Predictive & Pattern Analytics in zone: Analytical Intelligence\n",
      "2025-02-02 19:04:20,750 - INFO - Created category: Process Automation & Robotics in zone: Domain Specific\n",
      "2025-02-02 19:04:20,769 - INFO - Created category: Responsible AI Systems in zone: Enterprise Enablement\n",
      "2025-02-02 19:04:20,771 - INFO - Importing keywords and capabilities...\n",
      "2025-02-02 19:04:25,618 - INFO - Importing dependencies...\n",
      "2025-02-02 19:04:25,760 - INFO - Created dependency: AI Development & Operations (AIOps) -> data integration & management\n",
      "2025-02-02 19:04:25,773 - INFO - Created dependency: AI Development & Operations (AIOps) -> responsible ai systems\n",
      "2025-02-02 19:04:25,785 - INFO - Created dependency: Computer Vision & Media Analysis -> edge ai & iot\n",
      "2025-02-02 19:04:25,799 - INFO - Created dependency: Computer Vision & Media Analysis -> data integration & management\n",
      "2025-02-02 19:04:25,811 - INFO - Created dependency: Computer Vision & Media Analysis -> multimodal ai systems\n",
      "2025-02-02 19:04:25,828 - INFO - Created dependency: Cybersecurity & Threat Detection -> predictive & pattern analytics\n",
      "2025-02-02 19:04:25,845 - INFO - Created dependency: Cybersecurity & Threat Detection -> data integration & management\n",
      "2025-02-02 19:04:25,860 - INFO - Created dependency: Data Integration & Management -> responsible ai systems\n",
      "2025-02-02 19:04:25,872 - INFO - Created dependency: Decision Support & Optimization -> predictive & pattern analytics\n",
      "2025-02-02 19:04:25,881 - INFO - Created dependency: Decision Support & Optimization -> natural language processing\n",
      "2025-02-02 19:04:25,906 - INFO - Created dependency: Decision Support & Optimization -> data integration & management\n",
      "2025-02-02 19:04:25,948 - INFO - Created dependency: Edge AI & IoT -> data integration & management\n",
      "2025-02-02 19:04:25,960 - INFO - Created dependency: Edge AI & IoT -> ai development & operations\n",
      "2025-02-02 19:04:25,974 - INFO - Created dependency: Environmental & Geospatial AI -> computer vision & media analysis\n",
      "2025-02-02 19:04:25,994 - INFO - Created dependency: Environmental & Geospatial AI -> data integration & management\n",
      "2025-02-02 19:04:26,009 - INFO - Created dependency: Environmental & Geospatial AI -> edge ai & iot\n",
      "2025-02-02 19:04:26,022 - INFO - Created dependency: Healthcare & Biotech AI -> natural language processing\n",
      "2025-02-02 19:04:26,037 - INFO - Created dependency: Healthcare & Biotech AI -> computer vision & media analysis\n",
      "2025-02-02 19:04:26,055 - INFO - Created dependency: Healthcare & Biotech AI -> responsible ai systems\n",
      "2025-02-02 19:04:26,066 - INFO - Created dependency: Intelligent End-User Computing -> natural language processing\n",
      "2025-02-02 19:04:26,084 - INFO - Created dependency: Intelligent End-User Computing -> process automation & robotics\n",
      "2025-02-02 19:04:26,091 - INFO - Created dependency: Multimodal AI Systems -> natural language processing\n",
      "2025-02-02 19:04:26,104 - INFO - Created dependency: Multimodal AI Systems -> computer vision & media analysis\n",
      "2025-02-02 19:04:26,119 - INFO - Created dependency: Multimodal AI Systems -> data integration & management\n",
      "2025-02-02 19:04:26,131 - INFO - Created dependency: Natural Language Processing (NLP) -> data integration & management\n",
      "2025-02-02 19:04:26,145 - INFO - Created dependency: Natural Language Processing (NLP) -> responsible ai systems\n",
      "2025-02-02 19:04:26,158 - INFO - Created dependency: Predictive & Pattern Analytics -> data integration & management\n",
      "2025-02-02 19:04:26,172 - INFO - Created dependency: Process Automation & Robotics -> decision support & optimization\n",
      "2025-02-02 19:04:26,180 - INFO - Created dependency: Process Automation & Robotics -> natural language processing\n",
      "2025-02-02 19:04:26,191 - INFO - Created dependency: Responsible AI Systems -> data integration & management\n",
      "2025-02-02 19:04:26,201 - INFO - Created dependency: Responsible AI Systems -> ai development & operations\n",
      "2025-02-02 19:04:26,201 - INFO - Importing integration patterns...\n",
      "2025-02-02 19:04:27,060 - INFO - Running verification...\n",
      "2025-02-02 19:04:27,478 - INFO - Import completed successfully!\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    try:\n",
    "        # Verify connection and initialize components\n",
    "        if not verify_neo4j_connection():\n",
    "            raise ConnectionError(\"Failed to connect to Neo4j\")\n",
    "\n",
    "        tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "        data_prep = AITechDataPrep(CSV_PATH)\n",
    "        importer = AITechImporter(tx_manager)\n",
    "        verifier = ImportVerification(tx_manager)\n",
    "\n",
    "        # Import process\n",
    "        try:\n",
    "            # Stage 1: Preparation\n",
    "            df = data_prep.load_and_prepare()\n",
    "            tx_manager.create_constraints()\n",
    "\n",
    "            # Stage 2: Core Import\n",
    "            for stage in [\n",
    "                ('categories', importer.import_categories),\n",
    "                ('keywords and capabilities', importer.import_keywords_and_capabilities),\n",
    "                ('dependencies', importer.import_dependencies),\n",
    "                ('integration patterns', importer.import_integration_patterns)\n",
    "            ]:\n",
    "                logger.info(f\"Importing {stage[0]}...\")\n",
    "                stage[1](df)\n",
    "\n",
    "            # Stage 3: Verification\n",
    "            logger.info(\"Running verification...\")\n",
    "            verification_results = verifier.run_full_verification()\n",
    "            \n",
    "            if verification_results['status'] == 'FAIL':\n",
    "                # Log details before potential cleanup\n",
    "                logger.warning(\"Verification results:\")\n",
    "                for key, value in verification_results['details'].items():\n",
    "                    logger.warning(f\"{key}: {value}\")\n",
    "                \n",
    "                # Optional: Add manual override for dependency check\n",
    "                if (verification_results['details']['relationships']['status'] == 'FAIL' and\n",
    "                    all(r['keyword_count'] > 0 and r['capability_count'] > 0 and r['zone'] is not None\n",
    "                        for r in verification_results['details']['consistency']['details'])):\n",
    "                    logger.info(\"Override: Accepting import despite dependency verification failure\")\n",
    "                    return verification_results\n",
    "                \n",
    "                logger.error(\"Import verification failed\")\n",
    "                raise ValueError(\"Critical verification checks failed\")\n",
    "\n",
    "            logger.info(\"Import completed successfully!\")\n",
    "            return verification_results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Import failed: {str(e)}\")\n",
    "            logger.info(\"Attempting cleanup...\")\n",
    "            tx_manager.cleanup_database()\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Process failed: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        tx_manager.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2d7a83-f4b3-4c91-a962-c3564e6f52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITechQueries:\n",
    "    def __init__(self, tx_manager: Neo4jTransactionManager):\n",
    "        self.tx_manager = tx_manager\n",
    "\n",
    "    def find_by_keyword(self, keyword: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (k:Keyword {name: $keyword})<-[r:TAGGED_WITH]-(c:AICategory)\n",
    "        RETURN c.name as category, \n",
    "               c.category_definition as definition,\n",
    "               c.zone as zone,\n",
    "               r.weight as relevance\n",
    "        ORDER BY r.weight DESC\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {'keyword': keyword})\n",
    "\n",
    "    def get_category_capabilities(self, category: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "        RETURN cap.name as capability\n",
    "        ORDER BY capability\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {'category': category})\n",
    "\n",
    "    def get_dependency_chain(self, category: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH path = (c:AICategory {name: $category})-[:DEPENDS_ON*]->(dep:AICategory)\n",
    "        RETURN [node IN nodes(path) | node.name] as dependency_chain,\n",
    "               [node IN nodes(path) | node.zone] as zone_chain\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {'category': category})\n",
    "\n",
    "    def find_related_categories(self, category: str) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})-[:TAGGED_WITH]->(k:Keyword)\n",
    "        MATCH (k)<-[:TAGGED_WITH]-(related:AICategory)\n",
    "        WHERE related <> c\n",
    "        WITH related, count(k) as shared_keywords\n",
    "        RETURN related.name as category, \n",
    "               related.zone as zone,\n",
    "               shared_keywords,\n",
    "               related.maturity_level as maturity\n",
    "        ORDER BY shared_keywords DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {'category': category})\n",
    "\n",
    "    def get_maturity_distribution(self) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory)\n",
    "        RETURN c.maturity_level as level, \n",
    "               count(*) as count,\n",
    "               collect({name: c.name, zone: c.zone}) as categories\n",
    "        ORDER BY level\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {})\n",
    "\n",
    "    def get_categories_by_zone(self) -> List[Dict]:\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory)-[:BELONGS_TO]->(z:Zone)\n",
    "        RETURN z.name as zone,\n",
    "               collect({\n",
    "                   name: c.name,\n",
    "                   definition: c.category_definition,\n",
    "                   maturity: c.maturity_level\n",
    "               }) as categories\n",
    "        ORDER BY z.name\n",
    "        \"\"\"\n",
    "        return self.tx_manager.execute_read(query, {})\n",
    "\n",
    "    def get_category_details(self, category: str) -> Dict:\n",
    "        query = \"\"\"\n",
    "        MATCH (c:AICategory {name: $category})-[:BELONGS_TO]->(z:Zone)\n",
    "        OPTIONAL MATCH (c)-[:TAGGED_WITH]->(k:Keyword)\n",
    "        OPTIONAL MATCH (c)-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "        OPTIONAL MATCH (c)-[:DEPENDS_ON]->(d:AICategory)\n",
    "        RETURN c.name as name,\n",
    "               c.category_definition as definition,\n",
    "               c.maturity_level as maturity,\n",
    "               z.name as zone,\n",
    "               collect(DISTINCT k.name) as keywords,\n",
    "               collect(DISTINCT cap.name) as capabilities,\n",
    "               collect(DISTINCT d.name) as dependencies\n",
    "        \"\"\"\n",
    "        results = self.tx_manager.execute_read(query, {'category': category})\n",
    "        return results[0] if results else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d1ea96-7a08-4d19-b131-5141e6b88a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keyword search results:\n",
      "[{'category': 'Predictive & Pattern Analytics', 'definition': 'Systems that identify patterns in historical data to predict future outcomes and trends, supporting proactive decision-making and planning', 'zone': 'Analytical Intelligence', 'relevance': 1}]\n",
      "\n",
      "Related categories for NLP:\n",
      "[]\n",
      "\n",
      "Categories by Zone:\n",
      "[{'zone': 'Analytical Intelligence', 'categories': [{'maturity': 'mature', 'definition': 'Systems that identify patterns in historical data to predict future outcomes and trends, supporting proactive decision-making and planning', 'name': 'Predictive & Pattern Analytics'}, {'maturity': 'mature', 'definition': 'Systems that understand, analyze, and generate human language, enabling automated document processing, translation, and human-AI interaction', 'name': 'Natural Language Processing (NLP)'}, {'maturity': 'emerging', 'definition': 'Advanced AI systems that combine multiple types of input data (text, images, audio) to provide more comprehensive analysis and interaction capabilities', 'name': 'Multimodal AI Systems'}, {'maturity': 'established', 'definition': 'Systems that process and analyze visual information from images and videos, enabling automated identification, classification, and understanding of visual content', 'name': 'Computer Vision & Media Analysis'}]}, {'zone': 'Core Infrastructure', 'categories': [{'maturity': 'emerging', 'definition': 'Technologies that enable AI processing and decision-making directly on remote devices and sensors, reducing dependency on central computing resources', 'name': 'Edge AI & IoT'}, {'maturity': 'mature', 'definition': 'Systems for collecting, organizing, and preparing data across an organization to enable effective AI development and deployment', 'name': 'Data Integration & Management'}, {'maturity': 'established', 'definition': 'AI-powered systems that identify, analyze, and respond to security threats in real-time while protecting organizational assets and data according to federal security standards', 'name': 'Cybersecurity & Threat Detection'}, {'maturity': 'emerging', 'definition': 'Platforms and tools for developing, deploying, and managing AI systems reliably at scale, with emphasis on automation, monitoring, and governance', 'name': 'AI Development & Operations (AIOps)'}]}, {'zone': 'Domain Specific', 'categories': [{'maturity': 'established', 'definition': 'AI systems that control and optimize physical processes and robotic systems in manufacturing, maintenance, and other industrial applications', 'name': 'Process Automation & Robotics'}, {'maturity': 'established', 'definition': 'Specialized AI applications for healthcare delivery and biotech research, including medical imaging analysis, drug discovery, and patient care optimization', 'name': 'Healthcare & Biotech AI'}, {'maturity': 'emerging', 'definition': 'AI systems focused on analyzing environmental and location-based data to support climate, conservation, and urban planning decisions', 'name': 'Environmental & Geospatial AI'}]}, {'zone': 'Enterprise Enablement', 'categories': [{'maturity': 'emerging', 'definition': 'Frameworks and tools that ensure AI systems operate ethically and transparently, with built-in controls for fairness, accountability, and risk management', 'name': 'Responsible AI Systems'}, {'maturity': 'emerging', 'definition': 'AI-enabled end-user devices and software that provide local processing capabilities and intelligent features, enhancing user productivity while maintaining security and efficiency', 'name': 'Intelligent End-User Computing'}, {'maturity': 'established', 'definition': 'Systems that analyze complex business scenarios and provide data-driven recommendations to improve organizational decision-making and outcomes', 'name': 'Decision Support & Optimization'}]}]\n",
      "\n",
      "Detailed category information:\n",
      "{'name': 'Data Integration & Management', 'definition': 'Systems for collecting, organizing, and preparing data across an organization to enable effective AI development and deployment', 'maturity': 'mature', 'zone': 'Core Infrastructure', 'keywords': ['data integration', 'etl processing', 'data governance', 'data quality', 'master data', 'data pipeline', 'data catalog', 'metadata management', 'data lineage', 'data transformation', 'data validation', 'data orchestration', 'data standardization', 'data federation', 'data mesh', 'data fabric', 'data observability', 'data modeling', 'data security', 'data compliance'], 'capabilities': ['data integration', 'data transformation', 'data quality management', 'metadata management', 'data governance', 'pipeline orchestration', 'data cataloging', 'lineage tracking', 'data validation', 'schema management', 'data security', 'compliance monitoring', 'data standardization', 'data federation', 'data observability', 'data modeling'], 'dependencies': ['Responsible AI Systems']}\n"
     ]
    }
   ],
   "source": [
    "# First create the transaction manager\n",
    "tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# Then create queries object and execute\n",
    "queries = AITechQueries(tx_manager)\n",
    "\n",
    "# Example queries\n",
    "print(\"\\nKeyword search results:\")\n",
    "results = queries.find_by_keyword(\"machine learning\")\n",
    "print(results)\n",
    "\n",
    "print(\"\\nRelated categories for NLP:\")\n",
    "related = queries.find_related_categories(\"Natural Language Processing (NLP)\")\n",
    "print(related)\n",
    "\n",
    "print(\"\\nCategories by Zone:\")\n",
    "zone_results = queries.get_categories_by_zone()\n",
    "print(zone_results)\n",
    "\n",
    "print(\"\\nDetailed category information:\")\n",
    "category_details = queries.get_category_details(\"Data Integration & Management\")\n",
    "print(category_details)\n",
    "\n",
    "# Close connection when done\n",
    "tx_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a74c3b6e-1cbf-4f28-87e4-76680f52e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATURITY ANALYSIS:\n",
      "{'level': 'emerging', 'count': 6, 'zones': ['Core Infrastructure', 'Domain Specific', 'Enterprise Enablement', 'Analytical Intelligence']}\n",
      "{'level': 'established', 'count': 5, 'zones': ['Analytical Intelligence', 'Core Infrastructure', 'Enterprise Enablement', 'Domain Specific']}\n",
      "{'level': 'mature', 'count': 3, 'zones': ['Core Infrastructure', 'Analytical Intelligence']}\n",
      "\n",
      "KEYWORDS ANALYSIS:\n",
      "{'keyword': 'task automation', 'usage_count': 2, 'zones': ['Enterprise Enablement', 'Domain Specific']}\n",
      "{'keyword': 'feature extraction', 'usage_count': 2, 'zones': ['Analytical Intelligence']}\n",
      "{'keyword': 'sensor fusion', 'usage_count': 2, 'zones': ['Core Infrastructure', 'Analytical Intelligence']}\n",
      "{'keyword': 'workflow automation', 'usage_count': 2, 'zones': ['Enterprise Enablement', 'Domain Specific']}\n",
      "{'keyword': 'anomaly detection', 'usage_count': 2, 'zones': ['Core Infrastructure', 'Analytical Intelligence']}\n",
      "{'keyword': 'model governance', 'usage_count': 1, 'zones': ['Core Infrastructure']}\n",
      "{'keyword': 'ai pipeline management', 'usage_count': 1, 'zones': ['Core Infrastructure']}\n",
      "{'keyword': 'model management', 'usage_count': 1, 'zones': ['Core Infrastructure']}\n",
      "{'keyword': 'feature store', 'usage_count': 1, 'zones': ['Core Infrastructure']}\n",
      "{'keyword': 'model monitoring', 'usage_count': 1, 'zones': ['Core Infrastructure']}\n",
      "\n",
      "CATEGORIES ANALYSIS:\n",
      "{'category': 'AI Development & Operations (AIOps)', 'zone': 'Core Infrastructure', 'keyword_count': 25, 'capability_count': 21}\n",
      "{'category': 'Computer Vision & Media Analysis', 'zone': 'Analytical Intelligence', 'keyword_count': 20, 'capability_count': 15}\n",
      "{'category': 'Cybersecurity & Threat Detection', 'zone': 'Core Infrastructure', 'keyword_count': 20, 'capability_count': 17}\n",
      "{'category': 'Data Integration & Management', 'zone': 'Core Infrastructure', 'keyword_count': 20, 'capability_count': 16}\n",
      "{'category': 'Decision Support & Optimization', 'zone': 'Enterprise Enablement', 'keyword_count': 15, 'capability_count': 12}\n",
      "{'category': 'Edge AI & IoT', 'zone': 'Core Infrastructure', 'keyword_count': 17, 'capability_count': 14}\n",
      "{'category': 'Environmental & Geospatial AI', 'zone': 'Domain Specific', 'keyword_count': 15, 'capability_count': 13}\n",
      "{'category': 'Healthcare & Biotech AI', 'zone': 'Domain Specific', 'keyword_count': 17, 'capability_count': 13}\n",
      "{'category': 'Intelligent End-User Computing', 'zone': 'Enterprise Enablement', 'keyword_count': 14, 'capability_count': 10}\n",
      "{'category': 'Multimodal AI Systems', 'zone': 'Analytical Intelligence', 'keyword_count': 12, 'capability_count': 10}\n",
      "{'category': 'Natural Language Processing (NLP)', 'zone': 'Analytical Intelligence', 'keyword_count': 20, 'capability_count': 15}\n",
      "{'category': 'Predictive & Pattern Analytics', 'zone': 'Analytical Intelligence', 'keyword_count': 18, 'capability_count': 14}\n",
      "{'category': 'Process Automation & Robotics', 'zone': 'Domain Specific', 'keyword_count': 13, 'capability_count': 11}\n",
      "{'category': 'Responsible AI Systems', 'zone': 'Enterprise Enablement', 'keyword_count': 16, 'capability_count': 11}\n"
     ]
    }
   ],
   "source": [
    "# First create the transaction manager\n",
    "tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# Create verification queries\n",
    "verification_queries = {\n",
    "    'maturity': \"\"\"\n",
    "        MATCH (c:AICategory)\n",
    "        RETURN c.maturity_level as level, count(*) as count,\n",
    "               collect(DISTINCT c.zone) as zones;\n",
    "    \"\"\",\n",
    "    \n",
    "    'keywords': \"\"\"\n",
    "        MATCH (k:Keyword)<-[r:TAGGED_WITH]-(c:AICategory)\n",
    "        RETURN k.name as keyword, count(r) as usage_count,\n",
    "               collect(DISTINCT c.zone) as zones\n",
    "        ORDER BY usage_count DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\",\n",
    "    \n",
    "    'categories': \"\"\"\n",
    "        MATCH (c:AICategory)-[:BELONGS_TO]->(z:Zone)\n",
    "        OPTIONAL MATCH (c)-[:TAGGED_WITH]->(k:Keyword)\n",
    "        OPTIONAL MATCH (c)-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "        RETURN c.name as category,\n",
    "               z.name as zone,\n",
    "               count(DISTINCT k) as keyword_count,\n",
    "               count(DISTINCT cap) as capability_count\n",
    "        ORDER BY c.name;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Execute and display results\n",
    "with tx_manager.driver.session() as session:\n",
    "    for name, query in verification_queries.items():\n",
    "        print(f\"\\n{name.upper()} ANALYSIS:\")\n",
    "        results = session.run(query).data()\n",
    "        for result in results:\n",
    "            print(result)\n",
    "\n",
    "# Close connection when done\n",
    "tx_manager.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34993f5f-216d-4e32-a221-31d219d86728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI TECHNOLOGY CATEGORIES:\n",
      "\n",
      "\n",
      "=== Analytical Intelligence ===\n",
      "\n",
      "Category: Computer Vision & Media Analysis\n",
      "Definition: Systems that process and analyze visual information from images and videos, enabling automated identification, classification, and understanding of visual content\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Multimodal AI Systems\n",
      "Definition: Advanced AI systems that combine multiple types of input data (text, images, audio) to provide more comprehensive analysis and interaction capabilities\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Natural Language Processing (NLP)\n",
      "Definition: Systems that understand, analyze, and generate human language, enabling automated document processing, translation, and human-AI interaction\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Predictive & Pattern Analytics\n",
      "Definition: Systems that identify patterns in historical data to predict future outcomes and trends, supporting proactive decision-making and planning\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=== Core Infrastructure ===\n",
      "\n",
      "Category: AI Development & Operations (AIOps)\n",
      "Definition: Platforms and tools for developing, deploying, and managing AI systems reliably at scale, with emphasis on automation, monitoring, and governance\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Cybersecurity & Threat Detection\n",
      "Definition: AI-powered systems that identify, analyze, and respond to security threats in real-time while protecting organizational assets and data according to federal security standards\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Data Integration & Management\n",
      "Definition: Systems for collecting, organizing, and preparing data across an organization to enable effective AI development and deployment\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Edge AI & IoT\n",
      "Definition: Technologies that enable AI processing and decision-making directly on remote devices and sensors, reducing dependency on central computing resources\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=== Domain Specific ===\n",
      "\n",
      "Category: Environmental & Geospatial AI\n",
      "Definition: AI systems focused on analyzing environmental and location-based data to support climate, conservation, and urban planning decisions\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Healthcare & Biotech AI\n",
      "Definition: Specialized AI applications for healthcare delivery and biotech research, including medical imaging analysis, drug discovery, and patient care optimization\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Process Automation & Robotics\n",
      "Definition: AI systems that control and optimize physical processes and robotic systems in manufacturing, maintenance, and other industrial applications\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=== Enterprise Enablement ===\n",
      "\n",
      "Category: Decision Support & Optimization\n",
      "Definition: Systems that analyze complex business scenarios and provide data-driven recommendations to improve organizational decision-making and outcomes\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Intelligent End-User Computing\n",
      "Definition: AI-enabled end-user devices and software that provide local processing capabilities and intelligent features, enhancing user productivity while maintaining security and efficiency\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Category: Responsible AI Systems\n",
      "Definition: Frameworks and tools that ensure AI systems operate ethically and transparently, with built-in controls for fairness, accountability, and risk management\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First create the transaction manager\n",
    "tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# Query for categories, definitions, and zones\n",
    "categories_query = \"\"\"\n",
    "MATCH (c:AICategory)-[:BELONGS_TO]->(z:Zone)\n",
    "RETURN c.name as category, \n",
    "       c.category_definition as definition,\n",
    "       z.name as zone\n",
    "ORDER BY z.name, c.name;\n",
    "\"\"\"\n",
    "\n",
    "# Execute and display results\n",
    "with tx_manager.driver.session() as session:\n",
    "    print(\"AI TECHNOLOGY CATEGORIES:\\n\")\n",
    "    results = session.run(categories_query).data()\n",
    "    current_zone = None\n",
    "    for result in results:\n",
    "        # Print zone header when zone changes\n",
    "        if current_zone != result['zone']:\n",
    "            current_zone = result['zone']\n",
    "            print(f\"\\n=== {current_zone} ===\\n\")\n",
    "        \n",
    "        print(f\"Category: {result['category']}\")\n",
    "        print(f\"Definition: {result['definition']}\")\n",
    "        print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd07a24e-8c8e-4d07-94b1-33ad3b740a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CATEGORY: AI Development & Operations (AIOps)\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "Platforms and tools for developing, deploying, and managing AI systems reliably at scale, with emphasis on automation, monitoring, and governance\n",
      "\n",
      "KEYWORDS:\n",
      "mlops, devops for ai, model management, feature engineering, model monitoring, ai pipeline management, model governance, continuous integration, continuous deployment, feature store, model registry, experiment tracking, model serving, model versioning, reproducibility, containerization, orchestration, model lineage, compliance monitoring, drift detection, a/b testing, canary deployment, model observability, automated ml, pipeline automation\n",
      "\n",
      "CAPABILITIES:\n",
      "model development, model deployment, model monitoring, feature engineering, pipeline management, version control, experiment tracking, automated testing, deployment automation, model registry management, feature store operations, compliance validation, drift detection, performance optimization, resource scheduling, pipeline orchestration, container management, model serving, security scanning, audit logging, metadata management\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Computer Vision & Media Analysis\n",
      "MATURITY: established\n",
      "\n",
      "DEFINITION:\n",
      "Systems that process and analyze visual information from images and videos, enabling automated identification, classification, and understanding of visual content\n",
      "\n",
      "KEYWORDS:\n",
      "biometric analysis, visual anomaly detection, 3d reconstruction, depth sensing, thermal imaging, motion tracking, gesture recognition, optical character recognition, pose estimation, feature extraction, image segmentation, visual search, image classification, scene understanding, visual inspection, facial recognition, video analytics, object detection, image recognition, computer vision\n",
      "\n",
      "CAPABILITIES:\n",
      "object counting, automated inspection, visual authentication, perimeter monitoring, crowd analysis, visual search, motion analysis, scene understanding, document processing, satellite imagery analysis, medical imaging, quality inspection, object tracking, facial recognition, video surveillance\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Cybersecurity & Threat Detection\n",
      "MATURITY: established\n",
      "\n",
      "DEFINITION:\n",
      "AI-powered systems that identify, analyze, and respond to security threats in real-time while protecting organizational assets and data according to federal security standards\n",
      "\n",
      "KEYWORDS:\n",
      "risk analytics, attack surface, threat modeling, security orchestration, deception technology, zero trust, threat intelligence, security monitoring, vulnerability assessment, incident response, threat hunting, security automation, endpoint protection, network security, cyber intelligence, behavior analytics, malware analysis, intrusion detection, anomaly detection, threat detection\n",
      "\n",
      "CAPABILITIES:\n",
      "attack surface mapping, incident investigation, threat hunting, compliance monitoring, security orchestration, risk assessment, attack prevention, endpoint protection, network monitoring, threat intelligence, vulnerability scanning, incident response, security automation, behavioral analysis, malware prevention, intrusion detection, threat monitoring\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Data Integration & Management\n",
      "MATURITY: mature\n",
      "\n",
      "DEFINITION:\n",
      "Systems for collecting, organizing, and preparing data across an organization to enable effective AI development and deployment\n",
      "\n",
      "KEYWORDS:\n",
      "data integration, etl processing, data governance, data quality, master data, data pipeline, data catalog, metadata management, data lineage, data transformation, data validation, data orchestration, data standardization, data federation, data mesh, data fabric, data observability, data modeling, data security, data compliance\n",
      "\n",
      "CAPABILITIES:\n",
      "data integration, data transformation, data quality management, metadata management, data governance, pipeline orchestration, data cataloging, lineage tracking, data validation, schema management, data security, compliance monitoring, data standardization, data federation, data observability, data modeling\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Decision Support & Optimization\n",
      "MATURITY: established\n",
      "\n",
      "DEFINITION:\n",
      "Systems that analyze complex business scenarios and provide data-driven recommendations to improve organizational decision-making and outcomes\n",
      "\n",
      "KEYWORDS:\n",
      "simulation modeling, business rules, decision modeling, supply chain optimization, portfolio optimization, route optimization, scheduling optimization, resource optimization, risk analysis, scenario planning, constraint optimization, operational research, prescriptive analytics, predictive analytics, decision optimization\n",
      "\n",
      "CAPABILITIES:\n",
      "strategic planning, operational optimization, what-if analysis, simulation analysis, prescriptive analytics, predictive modeling, constraint solving, resource allocation, risk assessment, scenario planning, optimization analysis, decision modeling\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Edge AI & IoT\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "Technologies that enable AI processing and decision-making directly on remote devices and sensors, reducing dependency on central computing resources\n",
      "\n",
      "KEYWORDS:\n",
      "edge optimization, industrial iot, fog computing, edge intelligence, embedded systems, edge deployment, edge security, device management, edge orchestration, sensor fusion, real-time processing, edge analytics, distributed computing, embedded ai, sensor networks, iot devices, edge computing\n",
      "\n",
      "CAPABILITIES:\n",
      "edge synchronization, local inference, edge optimization, device monitoring, edge orchestration, embedded processing, edge security, distributed computing, sensor fusion, edge analytics, device management, real-time processing, sensor integration, edge deployment\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Environmental & Geospatial AI\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "AI systems focused on analyzing environmental and location-based data to support climate, conservation, and urban planning decisions\n",
      "\n",
      "KEYWORDS:\n",
      "biodiversity assessment, terrain analysis, spatial intelligence, climate prediction, resource management, pollution detection, environmental compliance, habitat monitoring, earth observation, geographic information, remote sensing, spatial analysis, climate modeling, environmental monitoring, geospatial analytics\n",
      "\n",
      "CAPABILITIES:\n",
      "change detection, risk modeling, impact assessment, terrain analysis, pollution detection, compliance monitoring, resource tracking, habitat assessment, remote sensing, spatial analytics, climate modeling, environmental monitoring, geospatial analysis\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Healthcare & Biotech AI\n",
      "MATURITY: established\n",
      "\n",
      "DEFINITION:\n",
      "Specialized AI applications for healthcare delivery and biotech research, including medical imaging analysis, drug discovery, and patient care optimization\n",
      "\n",
      "KEYWORDS:\n",
      "healthcare interoperability, precision medicine, clinical decision support, healthcare compliance, medical diagnosis, patient analytics, treatment optimization, disease prediction, biomarker detection, health records, clinical trials, drug discovery, patient monitoring, genomic analysis, diagnostic ai, clinical analytics, medical imaging\n",
      "\n",
      "CAPABILITIES:\n",
      "electronic health record processing, biomarker detection, clinical trials, patient analytics, treatment optimization, compliance validation, disease prediction, genomic analysis, clinical analytics, drug discovery, treatment planning, patient monitoring, medical diagnosis\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Intelligent End-User Computing\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "AI-enabled end-user devices and software that provide local processing capabilities and intelligent features, enhancing user productivity while maintaining security and efficiency\n",
      "\n",
      "KEYWORDS:\n",
      "digital assistance, workspace intelligence, behavioral analysis, user analytics, task automation, cognitive assistance, smart workspace, user experience, intelligent interfaces, document processing, workflow automation, productivity tools, user assistance, desktop automation\n",
      "\n",
      "CAPABILITIES:\n",
      "process automation, cognitive support, behavior analysis, workspace management, interface optimization, productivity enhancement, user assistance, document processing, workflow optimization, task automation\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Multimodal AI Systems\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "Advanced AI systems that combine multiple types of input data (text, images, audio) to provide more comprehensive analysis and interaction capabilities\n",
      "\n",
      "KEYWORDS:\n",
      "multimodal representation, cross-modal learning, multimodal interaction, modal synchronization, multimodal understanding, cross-modal integration, multimodal fusion, modal alignment, multi-input processing, sensor fusion, cross-modal analysis, multimodal learning\n",
      "\n",
      "CAPABILITIES:\n",
      "representation learning, multimodal integration, cross-modal learning, modal synchronization, interaction processing, multimodal understanding, modal alignment, sensor fusion, cross-modal analysis, multimodal processing\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Natural Language Processing (NLP)\n",
      "MATURITY: mature\n",
      "\n",
      "DEFINITION:\n",
      "Systems that understand, analyze, and generate human language, enabling automated document processing, translation, and human-AI interaction\n",
      "\n",
      "KEYWORDS:\n",
      "language generation, document classification, natural language inference, text mining, conversational ai, language modeling, syntactic analysis, semantic analysis, information extraction, question answering, text summarization, document understanding, machine translation, text generation, topic modeling, named entity recognition, text classification, sentiment analysis, language understanding, text analysis\n",
      "\n",
      "CAPABILITIES:\n",
      "conversation processing, language modeling, content classification, syntactic analysis, semantic processing, document analysis, question answering, summarization, translation, text generation, topic detection, entity extraction, sentiment analysis, language understanding, text processing\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Predictive & Pattern Analytics\n",
      "MATURITY: mature\n",
      "\n",
      "DEFINITION:\n",
      "Systems that identify patterns in historical data to predict future outcomes and trends, supporting proactive decision-making and planning\n",
      "\n",
      "KEYWORDS:\n",
      "outlier detection, sequence analysis, predictive maintenance, correlation analysis, dimensional reduction, feature extraction, machine learning, data mining, clustering, classification, regression analysis, trend analysis, anomaly detection, forecasting, time series analysis, statistical analysis, pattern recognition, predictive modeling\n",
      "\n",
      "CAPABILITIES:\n",
      "time series prediction, outlier detection, correlation discovery, sequence analysis, clustering, classification, feature analysis, data mining, statistical analysis, forecasting, anomaly detection, trend analysis, pattern detection, predictive modeling\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Process Automation & Robotics\n",
      "MATURITY: established\n",
      "\n",
      "DEFINITION:\n",
      "AI systems that control and optimize physical processes and robotic systems in manufacturing, maintenance, and other industrial applications\n",
      "\n",
      "KEYWORDS:\n",
      "automation orchestration, task mining, process mining, intelligent workflows, automated decision-making, process orchestration, digital workforce, cognitive automation, task automation, business process automation, intelligent automation, workflow automation, robotic process automation\n",
      "\n",
      "CAPABILITIES:\n",
      "automation analytics, exception handling, process monitoring, automation deployment, workflow management, task orchestration, process mining, decision automation, task automation, workflow optimization, process automation\n",
      "====================================================================================================\n",
      "\n",
      "CATEGORY: Responsible AI Systems\n",
      "MATURITY: emerging\n",
      "\n",
      "DEFINITION:\n",
      "Frameworks and tools that ensure AI systems operate ethically and transparently, with built-in controls for fairness, accountability, and risk management\n",
      "\n",
      "KEYWORDS:\n",
      "ai safety, privacy preservation, ai auditing, algorithmic fairness, ai compliance, model interpretability, bias mitigation, ai accountability, responsible ai, ethical ai, ai governance, explainable ai, model transparency, fairness metrics, bias detection, ai ethics\n",
      "\n",
      "CAPABILITIES:\n",
      "interpretability analysis, impact assessment, accountability tracking, governance enforcement, privacy protection, ethics validation, compliance monitoring, transparency analysis, model explanation, fairness assessment, bias detection\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# First create the transaction manager\n",
    "tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# Comprehensive category details query\n",
    "detailed_query = \"\"\"\n",
    "MATCH (c:AICategory)\n",
    "OPTIONAL MATCH (c)-[:TAGGED_WITH]->(k:Keyword)\n",
    "OPTIONAL MATCH (c)-[:HAS_CAPABILITY]->(cap:Capability)\n",
    "RETURN \n",
    "   c.name as category,\n",
    "   c.category_definition as definition,\n",
    "   c.maturity_level as maturity,\n",
    "   collect(DISTINCT k.name) as keywords,\n",
    "   collect(DISTINCT cap.name) as capabilities\n",
    "ORDER BY c.name;\n",
    "\"\"\"\n",
    "\n",
    "with tx_manager.driver.session() as session:\n",
    "   results = session.run(detailed_query).data()\n",
    "   for result in results:\n",
    "       print(f\"\\nCATEGORY: {result['category']}\")\n",
    "       print(f\"MATURITY: {result['maturity']}\")\n",
    "       print(f\"\\nDEFINITION:\")\n",
    "       print(result['definition'])\n",
    "       print(\"\\nKEYWORDS:\")\n",
    "       print(\", \".join(result['keywords']))\n",
    "       print(\"\\nCAPABILITIES:\")\n",
    "       print(\", \".join(result['capabilities']))\n",
    "       print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e48557-ed08-4909-a0f0-3d82dcd0ce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Check:\n",
      "AI Development & Operations (AIOps): ['Data Integration & Management', 'Responsible AI Systems']\n",
      "Computer Vision & Media Analysis: ['Multimodal AI Systems', 'Data Integration & Management', 'Edge AI & IoT']\n",
      "Cybersecurity & Threat Detection: ['Data Integration & Management', 'Predictive & Pattern Analytics']\n",
      "Data Integration & Management: ['Responsible AI Systems']\n",
      "Decision Support & Optimization: ['Data Integration & Management', 'Predictive & Pattern Analytics']\n",
      "Edge AI & IoT: ['Data Integration & Management']\n",
      "Environmental & Geospatial AI: ['Edge AI & IoT', 'Data Integration & Management', 'Computer Vision & Media Analysis']\n",
      "Healthcare & Biotech AI: ['Responsible AI Systems', 'Computer Vision & Media Analysis']\n",
      "Intelligent End-User Computing: ['Process Automation & Robotics']\n",
      "Multimodal AI Systems: ['Data Integration & Management', 'Computer Vision & Media Analysis']\n",
      "Natural Language Processing (NLP): ['Responsible AI Systems', 'Data Integration & Management']\n",
      "Predictive & Pattern Analytics: ['Data Integration & Management']\n",
      "Process Automation & Robotics: ['Decision Support & Optimization']\n",
      "Responsible AI Systems: ['Data Integration & Management']\n"
     ]
    }
   ],
   "source": [
    "# First create the transaction manager\n",
    "tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "dependency_check = \"\"\"\n",
    "MATCH (c:AICategory)\n",
    "OPTIONAL MATCH (c)-[:DEPENDS_ON]->(d:AICategory)\n",
    "RETURN c.name as category, collect(d.name) as dependencies;\n",
    "\"\"\"\n",
    "\n",
    "with tx_manager.driver.session() as session:\n",
    "    print(\"\\nDependency Check:\")\n",
    "    for result in session.run(dependency_check).data():\n",
    "        print(f\"{result['category']}: {result['dependencies']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb82e55-fe2e-4431-99b7-55d717e0c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE - the below will clear the whole database.  Leaving as commented out so it will not run\n",
    "\n",
    "# First create the transaction manager\n",
    "# tx_manager = Neo4jTransactionManager(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "# Clear database\n",
    "# tx_manager.cleanup_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112278d-f048-48fd-902b-b502712837ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neo4j",
   "language": "python",
   "name": "neo4j"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
